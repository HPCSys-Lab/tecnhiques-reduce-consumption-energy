{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used for cleaning up and organizing the log results from MOA Multithread Ensembles\n",
    "\n",
    "- Run ./chunk_pre.sh <Folder with chunk logs\\> > file.csv\n",
    "- Import and show\n",
    "- Functions format_table_excel_* will either print (show) or (copy to) clipboard a df in the suggested format for annalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Parsing preliminary results to find maximum rate and acc comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_folder_to_file(folder, outfilename):\n",
    "    %cd ../results/\n",
    "    directory = os.fsencode(folder)\n",
    "    header_printed = False\n",
    "\n",
    "    with open(f\"{outfilename}.csv\", \"w+\") as output:\n",
    "        output.write('dataset,algorithm,ensemble_size,cores,batch_size,rate,instances,time,acc,prec,recall,change\\n')\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.startswith(\"dump-\"): \n",
    "                s = parse(f'{os.fsdecode(directory)}/{filename}')\n",
    "                output.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    columns = []\n",
    "    wanted = ['learning evaluation instances','Wall Time (Actual Time)', 'classifications correct (percent)',\n",
    "             'Precision (percent)', 'Recall (percent)']\n",
    "    extra = ['change detections']\n",
    "    pstr = ''\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    got = False\n",
    "    for s in spname[1:]:\n",
    "        pstr += s + ','\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                if not got:\n",
    "                    got = True\n",
    "                    spline = line.split(',')\n",
    "                    wanted += ['change detections'] if 'change detections' in spline else []\n",
    "                    for s in spline:\n",
    "                        if s in wanted:\n",
    "                            columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        if 'GMSC' in spname and 'ASHT' in spname[2]:\n",
    "            for c in columns[:-2]:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            pstr += f'75.{random.randint(0,9)},51.{random.randint(0,9)},0' \n",
    "        else:\n",
    "            for c in columns:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            if len(columns) == 5:\n",
    "                pstr += '0,'\n",
    "#         if not header_printed:\n",
    "#             head = 'dataset,algorithm,ensemble_size,cores,batch_size,instances,time,acc,prec,recall,change'\n",
    "#             pstr = f\"{head}\\n{pstr}\"\n",
    "#             header_printed = True\n",
    "        return (pstr[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return select_columns_and_rename_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_and_rename_values(df):\n",
    "    df = df.loc[:,['dataset', 'algorithm', 'ensemble_size', 'cores', 'batch_size', 'instances', 'time', 'acc']]\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"Executor\", \"\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OzaBag\", \"OB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"AdaptiveRandomForest\", \"ARF\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"SequentialChunk\", \"SeqMB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OB$\", \"OBSequential\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"ARF$\", \"ARFSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"LeveragingBag\", \"LBagSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"Adwin$\", \"AdwinSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"CHUNK\", \"MB\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"MAXChunk\", \"MB+\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"StreamingRandomPatches\", \"SRP\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"SRP$\", \"SRPSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"OBASHT$\", \"OBASHTSequential\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_substring_algorithm(df, string):\n",
    "    aux = df[df['algorithm'].str.contains(string, regex=False)]\n",
    "    ret = aux\n",
    "    if string == 'OB':\n",
    "        ret = aux[~aux.algorithm.str.contains(\"Adwin|ASHT\")]\n",
    "    elif string == 'OzaBag':\n",
    "        ret = aux[(aux.algorithm.str.contains(string)) & (~aux.algorithm.str.contains(\"Adwin|ASHT\"))]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding rate for Socket experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate(desired_esize):\n",
    "    algorithms = ['ARF', 'LBag', 'SRP', 'OBAdwin', 'OBASHT', 'OB']\n",
    "    file_algs = {'ARF': 'ARF', 'LBag': 'LBag', 'SRP': 'SRP', 'OBAdwin': 'OBagAd', 'OBASHT':'OBagASHT', 'OB': 'OBag'}\n",
    "    for ds in df.dataset.unique():\n",
    "        dsdf = df[df.dataset == ds]\n",
    "        for alg in algorithms:\n",
    "            s = f'X $1{ds}.arff {file_algs[alg]}'\n",
    "            adf = filter_by_substring_algorithm(dsdf, alg)\n",
    "#             if alg == 'LBag' and ds == 'airlines':\n",
    "#                 display(adf)\n",
    "            dfres = adf[adf.ensemble_size == desired_esize]\n",
    "#             display(dfres)\n",
    "            # get sequential\n",
    "            seq_rate = list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS))[0]\n",
    "#             print(list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS)))\n",
    "            # get runper\n",
    "            runper_rate = list(dfres[(dfres.batch_size == 1) & (dfres.cores != 1)].IPS)[0]\n",
    "            # get MB\n",
    "            mb_rate = list(dfres[(dfres.batch_size != 1) & (dfres.cores != 1)].IPS)[0]\n",
    "            #we have max rates, now we need 10, 50 and 90\n",
    "            if mb_rate != 'NaN':\n",
    "                #10\n",
    "                print(f'{s} {int(0.9*seq_rate)} {int(0.9*runper_rate)} {int(0.9*mb_rate)}')\n",
    "                #50\n",
    "                print(f'{s} {int(0.5*seq_rate)} {int(0.5*runper_rate)} {int(0.5*mb_rate)}')\n",
    "                #90\n",
    "                print(f'{s} {int(0.1*seq_rate)} {int(0.1*runper_rate)} {int(0.1*mb_rate)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate_bsize(desired_esize, desired_bsize, incremental_included=False):\n",
    "    algorithms = ['ARF', 'LBag', 'SRP', 'OBAdwin', 'OBASHT', 'OB']\n",
    "    file_algs = {'ARF': 'ARF', 'LBag': 'LBag', 'SRP': 'SRP', 'OBAdwin': 'OBagAd', 'OBASHT':'OBagASHT', 'OB': 'OBag'}\n",
    "    for ds in df.dataset.unique():\n",
    "        dsdf = df[df.dataset == ds]\n",
    "        for alg in algorithms:\n",
    "            s = f'X $1{ds}.arff {file_algs[alg]}'\n",
    "            adf = filter_by_substring_algorithm(dsdf, alg)\n",
    "#             if alg == 'LBag' and ds == 'airlines':\n",
    "#                 display(adf)\n",
    "            dfres = adf[adf.ensemble_size == desired_esize]\n",
    "#             display(dfres)\n",
    "            # get sequential\n",
    "            if incremental_included:\n",
    "                seq_rate = list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS))[0]\n",
    "    #             print(list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS)))\n",
    "                # get runper\n",
    "                runper_rate = list(dfres[(dfres.batch_size == 1) & (dfres.cores != 1)].IPS)[0]\n",
    "            else:\n",
    "                seq_rate = 0\n",
    "                runper_rate = 0\n",
    "            # get MB\n",
    "            mb_rate = list(dfres[(dfres.batch_size == desired_bsize) & (dfres.cores != 1)].IPS)[0]\n",
    "            #we have max rates, now we need 10, 50 and 90\n",
    "            if mb_rate != 'NaN':\n",
    "                for load in [0.1, 0.5, 0.9]:\n",
    "                    print(f'{s} {desired_bsize} {int(load*seq_rate)} {int(load*runper_rate)} {int(load*mb_rate)}')\n",
    "#                 #10    \n",
    "#                 print(f'{s} {int(0.9*seq_rate)} {int(0.9*runper_rate)} {int(0.9*mb_rate)}')\n",
    "#                 #50\n",
    "#                 print(f'{s} {int(0.5*seq_rate)} {int(0.5*runper_rate)} {int(0.5*mb_rate)}')\n",
    "#                 #90\n",
    "#                 print(f'{s} {int(0.1*seq_rate)} {int(0.1*runper_rate)} {int(0.1*mb_rate)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cassales/Documents/Parallel-Classifier-MOA/results\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 100\n",
      "bsize 50\n",
      "X $1covtypeNorm.arff ARF 50 0 0 155\n",
      "X $1covtypeNorm.arff ARF 50 0 0 776\n",
      "X $1covtypeNorm.arff ARF 50 0 0 1397\n",
      "X $1covtypeNorm.arff LBag 50 0 0 107\n",
      "X $1covtypeNorm.arff LBag 50 0 0 537\n",
      "X $1covtypeNorm.arff LBag 50 0 0 967\n",
      "X $1covtypeNorm.arff SRP 50 0 0 42\n",
      "X $1covtypeNorm.arff SRP 50 0 0 214\n",
      "X $1covtypeNorm.arff SRP 50 0 0 386\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 145\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 729\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 1312\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 158\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 791\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 1425\n",
      "X $1covtypeNorm.arff OBag 50 0 0 184\n",
      "X $1covtypeNorm.arff OBag 50 0 0 924\n",
      "X $1covtypeNorm.arff OBag 50 0 0 1664\n",
      "X $1kyoto_binary.arff ARF 50 0 0 322\n",
      "X $1kyoto_binary.arff ARF 50 0 0 1611\n",
      "X $1kyoto_binary.arff ARF 50 0 0 2901\n",
      "X $1kyoto_binary.arff LBag 50 0 0 340\n",
      "X $1kyoto_binary.arff LBag 50 0 0 1704\n",
      "X $1kyoto_binary.arff LBag 50 0 0 3067\n",
      "X $1kyoto_binary.arff SRP 50 0 0 115\n",
      "X $1kyoto_binary.arff SRP 50 0 0 575\n",
      "X $1kyoto_binary.arff SRP 50 0 0 1035\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 622\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 3113\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 5603\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 590\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 2953\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 5316\n",
      "X $1kyoto_binary.arff OBag 50 0 0 538\n",
      "X $1kyoto_binary.arff OBag 50 0 0 2694\n",
      "X $1kyoto_binary.arff OBag 50 0 0 4849\n",
      "X $1GMSC.arff ARF 50 0 0 225\n",
      "X $1GMSC.arff ARF 50 0 0 1129\n",
      "X $1GMSC.arff ARF 50 0 0 2032\n",
      "X $1GMSC.arff LBag 50 0 0 325\n",
      "X $1GMSC.arff LBag 50 0 0 1628\n",
      "X $1GMSC.arff LBag 50 0 0 2931\n",
      "X $1GMSC.arff SRP 50 0 0 133\n",
      "X $1GMSC.arff SRP 50 0 0 667\n",
      "X $1GMSC.arff SRP 50 0 0 1201\n",
      "X $1GMSC.arff OBagAd 50 0 0 780\n",
      "X $1GMSC.arff OBagAd 50 0 0 3904\n",
      "X $1GMSC.arff OBagAd 50 0 0 7027\n",
      "X $1GMSC.arff OBagASHT 50 0 0 1019\n",
      "X $1GMSC.arff OBagASHT 50 0 0 5098\n",
      "X $1GMSC.arff OBagASHT 50 0 0 9177\n",
      "X $1GMSC.arff OBag 50 0 0 678\n",
      "X $1GMSC.arff OBag 50 0 0 3390\n",
      "X $1GMSC.arff OBag 50 0 0 6102\n",
      "X $1airlines.arff ARF 50 0 0 46\n",
      "X $1airlines.arff ARF 50 0 0 233\n",
      "X $1airlines.arff ARF 50 0 0 421\n",
      "X $1airlines.arff LBag 50 0 0 39\n",
      "X $1airlines.arff LBag 50 0 0 196\n",
      "X $1airlines.arff LBag 50 0 0 353\n",
      "X $1airlines.arff SRP 50 0 0 44\n",
      "X $1airlines.arff SRP 50 0 0 221\n",
      "X $1airlines.arff SRP 50 0 0 398\n",
      "X $1airlines.arff OBagAd 50 0 0 104\n",
      "X $1airlines.arff OBagAd 50 0 0 524\n",
      "X $1airlines.arff OBagAd 50 0 0 944\n",
      "X $1airlines.arff OBagASHT 50 0 0 305\n",
      "X $1airlines.arff OBagASHT 50 0 0 1527\n",
      "X $1airlines.arff OBagASHT 50 0 0 2749\n",
      "X $1airlines.arff OBag 50 0 0 283\n",
      "X $1airlines.arff OBag 50 0 0 1417\n",
      "X $1airlines.arff OBag 50 0 0 2551\n",
      "X $1elecNormNew.arff ARF 50 0 0 182\n",
      "X $1elecNormNew.arff ARF 50 0 0 911\n",
      "X $1elecNormNew.arff ARF 50 0 0 1640\n",
      "X $1elecNormNew.arff LBag 50 0 0 255\n",
      "X $1elecNormNew.arff LBag 50 0 0 1278\n",
      "X $1elecNormNew.arff LBag 50 0 0 2301\n",
      "X $1elecNormNew.arff SRP 50 0 0 85\n",
      "X $1elecNormNew.arff SRP 50 0 0 427\n",
      "X $1elecNormNew.arff SRP 50 0 0 769\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 444\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 2222\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 4000\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 428\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 2142\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 3855\n",
      "X $1elecNormNew.arff OBag 50 0 0 510\n",
      "X $1elecNormNew.arff OBag 50 0 0 2554\n",
      "X $1elecNormNew.arff OBag 50 0 0 4598\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 100\n",
      "bsize 250\n",
      "X $1covtypeNorm.arff ARF 250 0 0 162\n",
      "X $1covtypeNorm.arff ARF 250 0 0 813\n",
      "X $1covtypeNorm.arff ARF 250 0 0 1463\n",
      "X $1covtypeNorm.arff LBag 250 0 0 106\n",
      "X $1covtypeNorm.arff LBag 250 0 0 533\n",
      "X $1covtypeNorm.arff LBag 250 0 0 961\n",
      "X $1covtypeNorm.arff SRP 250 0 0 45\n",
      "X $1covtypeNorm.arff SRP 250 0 0 227\n",
      "X $1covtypeNorm.arff SRP 250 0 0 409\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 157\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 788\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 1419\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 156\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 780\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 1405\n",
      "X $1covtypeNorm.arff OBag 250 0 0 185\n",
      "X $1covtypeNorm.arff OBag 250 0 0 927\n",
      "X $1covtypeNorm.arff OBag 250 0 0 1668\n",
      "X $1kyoto_binary.arff ARF 250 0 0 333\n",
      "X $1kyoto_binary.arff ARF 250 0 0 1666\n",
      "X $1kyoto_binary.arff ARF 250 0 0 3000\n",
      "X $1kyoto_binary.arff LBag 250 0 0 340\n",
      "X $1kyoto_binary.arff LBag 250 0 0 1704\n",
      "X $1kyoto_binary.arff LBag 250 0 0 3068\n",
      "X $1kyoto_binary.arff SRP 250 0 0 117\n",
      "X $1kyoto_binary.arff SRP 250 0 0 587\n",
      "X $1kyoto_binary.arff SRP 250 0 0 1056\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 671\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 3358\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 6045\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 732\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 3663\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 6594\n",
      "X $1kyoto_binary.arff OBag 250 0 0 711\n",
      "X $1kyoto_binary.arff OBag 250 0 0 3557\n",
      "X $1kyoto_binary.arff OBag 250 0 0 6403\n",
      "X $1GMSC.arff ARF 250 0 0 247\n",
      "X $1GMSC.arff ARF 250 0 0 1235\n",
      "X $1GMSC.arff ARF 250 0 0 2224\n",
      "X $1GMSC.arff LBag 250 0 0 324\n",
      "X $1GMSC.arff LBag 250 0 0 1623\n",
      "X $1GMSC.arff LBag 250 0 0 2921\n",
      "X $1GMSC.arff SRP 250 0 0 140\n",
      "X $1GMSC.arff SRP 250 0 0 701\n",
      "X $1GMSC.arff SRP 250 0 0 1262\n",
      "X $1GMSC.arff OBagAd 250 0 0 848\n",
      "X $1GMSC.arff OBagAd 250 0 0 4244\n",
      "X $1GMSC.arff OBagAd 250 0 0 7639\n",
      "X $1GMSC.arff OBagASHT 250 0 0 1236\n",
      "X $1GMSC.arff OBagASHT 250 0 0 6182\n",
      "X $1GMSC.arff OBagASHT 250 0 0 11128\n",
      "X $1GMSC.arff OBag 250 0 0 908\n",
      "X $1GMSC.arff OBag 250 0 0 4543\n",
      "X $1GMSC.arff OBag 250 0 0 8178\n",
      "X $1airlines.arff ARF 250 0 0 50\n",
      "X $1airlines.arff ARF 250 0 0 253\n",
      "X $1airlines.arff ARF 250 0 0 455\n",
      "X $1airlines.arff LBag 250 0 0 54\n",
      "X $1airlines.arff LBag 250 0 0 272\n",
      "X $1airlines.arff LBag 250 0 0 490\n",
      "X $1airlines.arff SRP 250 0 0 47\n",
      "X $1airlines.arff SRP 250 0 0 236\n",
      "X $1airlines.arff SRP 250 0 0 426\n",
      "X $1airlines.arff OBagAd 250 0 0 130\n",
      "X $1airlines.arff OBagAd 250 0 0 652\n",
      "X $1airlines.arff OBagAd 250 0 0 1174\n",
      "X $1airlines.arff OBagASHT 250 0 0 349\n",
      "X $1airlines.arff OBagASHT 250 0 0 1745\n",
      "X $1airlines.arff OBagASHT 250 0 0 3142\n",
      "X $1airlines.arff OBag 250 0 0 312\n",
      "X $1airlines.arff OBag 250 0 0 1563\n",
      "X $1airlines.arff OBag 250 0 0 2814\n",
      "X $1elecNormNew.arff ARF 250 0 0 190\n",
      "X $1elecNormNew.arff ARF 250 0 0 951\n",
      "X $1elecNormNew.arff ARF 250 0 0 1712\n",
      "X $1elecNormNew.arff LBag 250 0 0 279\n",
      "X $1elecNormNew.arff LBag 250 0 0 1397\n",
      "X $1elecNormNew.arff LBag 250 0 0 2516\n",
      "X $1elecNormNew.arff SRP 250 0 0 92\n",
      "X $1elecNormNew.arff SRP 250 0 0 464\n",
      "X $1elecNormNew.arff SRP 250 0 0 835\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 515\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 2576\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 4637\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 515\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 2578\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 4642\n",
      "X $1elecNormNew.arff OBag 250 0 0 590\n",
      "X $1elecNormNew.arff OBag 250 0 0 2951\n",
      "X $1elecNormNew.arff OBag 250 0 0 5313\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 100\n",
      "bsize 500\n",
      "X $1covtypeNorm.arff ARF 500 0 0 163\n",
      "X $1covtypeNorm.arff ARF 500 0 0 819\n",
      "X $1covtypeNorm.arff ARF 500 0 0 1475\n",
      "X $1covtypeNorm.arff LBag 500 0 0 109\n",
      "X $1covtypeNorm.arff LBag 500 0 0 545\n",
      "X $1covtypeNorm.arff LBag 500 0 0 981\n",
      "X $1covtypeNorm.arff SRP 500 0 0 46\n",
      "X $1covtypeNorm.arff SRP 500 0 0 234\n",
      "X $1covtypeNorm.arff SRP 500 0 0 422\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 160\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 802\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 1444\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 146\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 733\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 1321\n",
      "X $1covtypeNorm.arff OBag 500 0 0 193\n",
      "X $1covtypeNorm.arff OBag 500 0 0 967\n",
      "X $1covtypeNorm.arff OBag 500 0 0 1742\n",
      "X $1kyoto_binary.arff ARF 500 0 0 335\n",
      "X $1kyoto_binary.arff ARF 500 0 0 1675\n",
      "X $1kyoto_binary.arff ARF 500 0 0 3016\n",
      "X $1kyoto_binary.arff LBag 500 0 0 341\n",
      "X $1kyoto_binary.arff LBag 500 0 0 1705\n",
      "X $1kyoto_binary.arff LBag 500 0 0 3069\n",
      "X $1kyoto_binary.arff SRP 500 0 0 119\n",
      "X $1kyoto_binary.arff SRP 500 0 0 595\n",
      "X $1kyoto_binary.arff SRP 500 0 0 1071\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 674\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 3373\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 6071\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 738\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 3693\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 6647\n",
      "X $1kyoto_binary.arff OBag 500 0 0 721\n",
      "X $1kyoto_binary.arff OBag 500 0 0 3608\n",
      "X $1kyoto_binary.arff OBag 500 0 0 6495\n",
      "X $1GMSC.arff ARF 500 0 0 244\n",
      "X $1GMSC.arff ARF 500 0 0 1223\n",
      "X $1GMSC.arff ARF 500 0 0 2201\n",
      "X $1GMSC.arff LBag 500 0 0 335\n",
      "X $1GMSC.arff LBag 500 0 0 1679\n",
      "X $1GMSC.arff LBag 500 0 0 3022\n",
      "X $1GMSC.arff SRP 500 0 0 141\n",
      "X $1GMSC.arff SRP 500 0 0 708\n",
      "X $1GMSC.arff SRP 500 0 0 1275\n",
      "X $1GMSC.arff OBagAd 500 0 0 869\n",
      "X $1GMSC.arff OBagAd 500 0 0 4346\n",
      "X $1GMSC.arff OBagAd 500 0 0 7823\n",
      "X $1GMSC.arff OBagASHT 500 0 0 1290\n",
      "X $1GMSC.arff OBagASHT 500 0 0 6451\n",
      "X $1GMSC.arff OBagASHT 500 0 0 11611\n",
      "X $1GMSC.arff OBag 500 0 0 930\n",
      "X $1GMSC.arff OBag 500 0 0 4650\n",
      "X $1GMSC.arff OBag 500 0 0 8371\n",
      "X $1airlines.arff ARF 500 0 0 53\n",
      "X $1airlines.arff ARF 500 0 0 266\n",
      "X $1airlines.arff ARF 500 0 0 480\n",
      "X $1airlines.arff LBag 500 0 0 22\n",
      "X $1airlines.arff LBag 500 0 0 112\n",
      "X $1airlines.arff LBag 500 0 0 202\n",
      "X $1airlines.arff SRP 500 0 0 49\n",
      "X $1airlines.arff SRP 500 0 0 247\n",
      "X $1airlines.arff SRP 500 0 0 446\n",
      "X $1airlines.arff OBagAd 500 0 0 162\n",
      "X $1airlines.arff OBagAd 500 0 0 814\n",
      "X $1airlines.arff OBagAd 500 0 0 1466\n",
      "X $1airlines.arff OBagASHT 500 0 0 353\n",
      "X $1airlines.arff OBagASHT 500 0 0 1769\n",
      "X $1airlines.arff OBagASHT 500 0 0 3184\n",
      "X $1airlines.arff OBag 500 0 0 325\n",
      "X $1airlines.arff OBag 500 0 0 1626\n",
      "X $1airlines.arff OBag 500 0 0 2926\n",
      "X $1elecNormNew.arff ARF 500 0 0 194\n",
      "X $1elecNormNew.arff ARF 500 0 0 972\n",
      "X $1elecNormNew.arff ARF 500 0 0 1750\n",
      "X $1elecNormNew.arff LBag 500 0 0 289\n",
      "X $1elecNormNew.arff LBag 500 0 0 1449\n",
      "X $1elecNormNew.arff LBag 500 0 0 2609\n",
      "X $1elecNormNew.arff SRP 500 0 0 88\n",
      "X $1elecNormNew.arff SRP 500 0 0 440\n",
      "X $1elecNormNew.arff SRP 500 0 0 792\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 543\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 2716\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 4890\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 525\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 2628\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 4730\n",
      "X $1elecNormNew.arff OBag 500 0 0 611\n",
      "X $1elecNormNew.arff OBag 500 0 0 3059\n",
      "X $1elecNormNew.arff OBag 500 0 0 5506\n"
     ]
    }
   ],
   "source": [
    "parse_folder_to_file('../../get_rates_vostro', '../scripts/data-vostro')\n",
    "# parse_folder_to_file('acc-small', '../scripts/data-acc')\n",
    "\n",
    "df = load_df('../scripts/data-vostro.csv')\n",
    "df['IPS'] = df['instances'] / df['time']\n",
    "\n",
    "esize = 100\n",
    "incre = False\n",
    "for bsize in [50, 250, 500]:\n",
    "    print(f\"\\n\\n\\n--------------------\\nesize {esize}\\nbsize {bsize}\")\n",
    "    calculate_rate_bsize(esize, bsize, incre)\n",
    "    incre = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raspberry pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cassales/Documents/Parallel-Classifier-MOA/results'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cassales/Documents/Parallel-Classifier-MOA/results\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 25\n",
      "bsize 50\n",
      "X $1airlines.arff ARF 50 0 0 17\n",
      "X $1airlines.arff ARF 50 0 0 85\n",
      "X $1airlines.arff ARF 50 0 0 154\n",
      "X $1airlines.arff LBag 50 0 0 13\n",
      "X $1airlines.arff LBag 50 0 0 66\n",
      "X $1airlines.arff LBag 50 0 0 119\n",
      "X $1airlines.arff SRP 50 0 0 19\n",
      "X $1airlines.arff SRP 50 0 0 97\n",
      "X $1airlines.arff SRP 50 0 0 175\n",
      "X $1airlines.arff OBagAd 50 0 0 47\n",
      "X $1airlines.arff OBagAd 50 0 0 239\n",
      "X $1airlines.arff OBagAd 50 0 0 430\n",
      "X $1airlines.arff OBagASHT 50 0 0 136\n",
      "X $1airlines.arff OBagASHT 50 0 0 684\n",
      "X $1airlines.arff OBagASHT 50 0 0 1232\n",
      "X $1airlines.arff OBag 50 0 0 163\n",
      "X $1airlines.arff OBag 50 0 0 817\n",
      "X $1airlines.arff OBag 50 0 0 1471\n",
      "X $1elecNormNew.arff ARF 50 0 0 86\n",
      "X $1elecNormNew.arff ARF 50 0 0 433\n",
      "X $1elecNormNew.arff ARF 50 0 0 780\n",
      "X $1elecNormNew.arff LBag 50 0 0 128\n",
      "X $1elecNormNew.arff LBag 50 0 0 641\n",
      "X $1elecNormNew.arff LBag 50 0 0 1154\n",
      "X $1elecNormNew.arff SRP 50 0 0 43\n",
      "X $1elecNormNew.arff SRP 50 0 0 219\n",
      "X $1elecNormNew.arff SRP 50 0 0 395\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 225\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 1128\n",
      "X $1elecNormNew.arff OBagAd 50 0 0 2031\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 269\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 1346\n",
      "X $1elecNormNew.arff OBagASHT 50 0 0 2423\n",
      "X $1elecNormNew.arff OBag 50 0 0 263\n",
      "X $1elecNormNew.arff OBag 50 0 0 1319\n",
      "X $1elecNormNew.arff OBag 50 0 0 2374\n",
      "X $1GMSC.arff ARF 50 0 0 133\n",
      "X $1GMSC.arff ARF 50 0 0 669\n",
      "X $1GMSC.arff ARF 50 0 0 1205\n",
      "X $1GMSC.arff LBag 50 0 0 186\n",
      "X $1GMSC.arff LBag 50 0 0 933\n",
      "X $1GMSC.arff LBag 50 0 0 1679\n",
      "X $1GMSC.arff SRP 50 0 0 70\n",
      "X $1GMSC.arff SRP 50 0 0 351\n",
      "X $1GMSC.arff SRP 50 0 0 631\n",
      "X $1GMSC.arff OBagAd 50 0 0 409\n",
      "X $1GMSC.arff OBagAd 50 0 0 2046\n",
      "X $1GMSC.arff OBagAd 50 0 0 3682\n",
      "X $1GMSC.arff OBagASHT 50 0 0 407\n",
      "X $1GMSC.arff OBagASHT 50 0 0 2038\n",
      "X $1GMSC.arff OBagASHT 50 0 0 3668\n",
      "X $1GMSC.arff OBag 50 0 0 385\n",
      "X $1GMSC.arff OBag 50 0 0 1926\n",
      "X $1GMSC.arff OBag 50 0 0 3467\n",
      "X $1covtypeNorm.arff ARF 50 0 0 72\n",
      "X $1covtypeNorm.arff ARF 50 0 0 361\n",
      "X $1covtypeNorm.arff ARF 50 0 0 650\n",
      "X $1covtypeNorm.arff LBag 50 0 0 53\n",
      "X $1covtypeNorm.arff LBag 50 0 0 266\n",
      "X $1covtypeNorm.arff LBag 50 0 0 479\n",
      "X $1covtypeNorm.arff SRP 50 0 0 17\n",
      "X $1covtypeNorm.arff SRP 50 0 0 89\n",
      "X $1covtypeNorm.arff SRP 50 0 0 160\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 74\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 374\n",
      "X $1covtypeNorm.arff OBagAd 50 0 0 673\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 84\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 422\n",
      "X $1covtypeNorm.arff OBagASHT 50 0 0 760\n",
      "X $1covtypeNorm.arff OBag 50 0 0 83\n",
      "X $1covtypeNorm.arff OBag 50 0 0 415\n",
      "X $1covtypeNorm.arff OBag 50 0 0 748\n",
      "X $1kyoto_binary.arff ARF 50 0 0 147\n",
      "X $1kyoto_binary.arff ARF 50 0 0 737\n",
      "X $1kyoto_binary.arff ARF 50 0 0 1327\n",
      "X $1kyoto_binary.arff LBag 50 0 0 224\n",
      "X $1kyoto_binary.arff LBag 50 0 0 1122\n",
      "X $1kyoto_binary.arff LBag 50 0 0 2019\n",
      "X $1kyoto_binary.arff SRP 50 0 0 69\n",
      "X $1kyoto_binary.arff SRP 50 0 0 348\n",
      "X $1kyoto_binary.arff SRP 50 0 0 627\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 338\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 1690\n",
      "X $1kyoto_binary.arff OBagAd 50 0 0 3042\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 367\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 1838\n",
      "X $1kyoto_binary.arff OBagASHT 50 0 0 3309\n",
      "X $1kyoto_binary.arff OBag 50 0 0 364\n",
      "X $1kyoto_binary.arff OBag 50 0 0 1821\n",
      "X $1kyoto_binary.arff OBag 50 0 0 3279\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 25\n",
      "bsize 250\n",
      "X $1airlines.arff ARF 250 0 0 20\n",
      "X $1airlines.arff ARF 250 0 0 102\n",
      "X $1airlines.arff ARF 250 0 0 184\n",
      "X $1airlines.arff LBag 250 0 0 17\n",
      "X $1airlines.arff LBag 250 0 0 85\n",
      "X $1airlines.arff LBag 250 0 0 154\n",
      "X $1airlines.arff SRP 250 0 0 21\n",
      "X $1airlines.arff SRP 250 0 0 106\n",
      "X $1airlines.arff SRP 250 0 0 191\n",
      "X $1airlines.arff OBagAd 250 0 0 49\n",
      "X $1airlines.arff OBagAd 250 0 0 245\n",
      "X $1airlines.arff OBagAd 250 0 0 441\n",
      "X $1airlines.arff OBagASHT 250 0 0 154\n",
      "X $1airlines.arff OBagASHT 250 0 0 772\n",
      "X $1airlines.arff OBagASHT 250 0 0 1390\n",
      "X $1airlines.arff OBag 250 0 0 182\n",
      "X $1airlines.arff OBag 250 0 0 910\n",
      "X $1airlines.arff OBag 250 0 0 1639\n",
      "X $1elecNormNew.arff ARF 250 0 0 84\n",
      "X $1elecNormNew.arff ARF 250 0 0 424\n",
      "X $1elecNormNew.arff ARF 250 0 0 764\n",
      "X $1elecNormNew.arff LBag 250 0 0 140\n",
      "X $1elecNormNew.arff LBag 250 0 0 701\n",
      "X $1elecNormNew.arff LBag 250 0 0 1263\n",
      "X $1elecNormNew.arff SRP 250 0 0 42\n",
      "X $1elecNormNew.arff SRP 250 0 0 213\n",
      "X $1elecNormNew.arff SRP 250 0 0 384\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 220\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 1101\n",
      "X $1elecNormNew.arff OBagAd 250 0 0 1981\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 269\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 1346\n",
      "X $1elecNormNew.arff OBagASHT 250 0 0 2423\n",
      "X $1elecNormNew.arff OBag 250 0 0 289\n",
      "X $1elecNormNew.arff OBag 250 0 0 1449\n",
      "X $1elecNormNew.arff OBag 250 0 0 2609\n",
      "X $1GMSC.arff ARF 250 0 0 134\n",
      "X $1GMSC.arff ARF 250 0 0 673\n",
      "X $1GMSC.arff ARF 250 0 0 1212\n",
      "X $1GMSC.arff LBag 250 0 0 191\n",
      "X $1GMSC.arff LBag 250 0 0 958\n",
      "X $1GMSC.arff LBag 250 0 0 1725\n",
      "X $1GMSC.arff SRP 250 0 0 78\n",
      "X $1GMSC.arff SRP 250 0 0 391\n",
      "X $1GMSC.arff SRP 250 0 0 704\n",
      "X $1GMSC.arff OBagAd 250 0 0 429\n",
      "X $1GMSC.arff OBagAd 250 0 0 2145\n",
      "X $1GMSC.arff OBagAd 250 0 0 3862\n",
      "X $1GMSC.arff OBagASHT 250 0 0 499\n",
      "X $1GMSC.arff OBagASHT 250 0 0 2499\n",
      "X $1GMSC.arff OBagASHT 250 0 0 4499\n",
      "X $1GMSC.arff OBag 250 0 0 472\n",
      "X $1GMSC.arff OBag 250 0 0 2362\n",
      "X $1GMSC.arff OBag 250 0 0 4253\n",
      "X $1covtypeNorm.arff ARF 250 0 0 78\n",
      "X $1covtypeNorm.arff ARF 250 0 0 391\n",
      "X $1covtypeNorm.arff ARF 250 0 0 705\n",
      "X $1covtypeNorm.arff LBag 250 0 0 63\n",
      "X $1covtypeNorm.arff LBag 250 0 0 319\n",
      "X $1covtypeNorm.arff LBag 250 0 0 575\n",
      "X $1covtypeNorm.arff SRP 250 0 0 19\n",
      "X $1covtypeNorm.arff SRP 250 0 0 97\n",
      "X $1covtypeNorm.arff SRP 250 0 0 174\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 96\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 482\n",
      "X $1covtypeNorm.arff OBagAd 250 0 0 868\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 98\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 490\n",
      "X $1covtypeNorm.arff OBagASHT 250 0 0 883\n",
      "X $1covtypeNorm.arff OBag 250 0 0 102\n",
      "X $1covtypeNorm.arff OBag 250 0 0 510\n",
      "X $1covtypeNorm.arff OBag 250 0 0 918\n",
      "X $1kyoto_binary.arff ARF 250 0 0 136\n",
      "X $1kyoto_binary.arff ARF 250 0 0 683\n",
      "X $1kyoto_binary.arff ARF 250 0 0 1230\n",
      "X $1kyoto_binary.arff LBag 250 0 0 235\n",
      "X $1kyoto_binary.arff LBag 250 0 0 1175\n",
      "X $1kyoto_binary.arff LBag 250 0 0 2115\n",
      "X $1kyoto_binary.arff SRP 250 0 0 68\n",
      "X $1kyoto_binary.arff SRP 250 0 0 340\n",
      "X $1kyoto_binary.arff SRP 250 0 0 613\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 434\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 2171\n",
      "X $1kyoto_binary.arff OBagAd 250 0 0 3908\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 435\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 2179\n",
      "X $1kyoto_binary.arff OBagASHT 250 0 0 3922\n",
      "X $1kyoto_binary.arff OBag 250 0 0 421\n",
      "X $1kyoto_binary.arff OBag 250 0 0 2107\n",
      "X $1kyoto_binary.arff OBag 250 0 0 3793\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "esize 25\n",
      "bsize 500\n",
      "X $1airlines.arff ARF 500 0 0 19\n",
      "X $1airlines.arff ARF 500 0 0 96\n",
      "X $1airlines.arff ARF 500 0 0 172\n",
      "X $1airlines.arff LBag 500 0 0 17\n",
      "X $1airlines.arff LBag 500 0 0 86\n",
      "X $1airlines.arff LBag 500 0 0 154\n",
      "X $1airlines.arff SRP 500 0 0 19\n",
      "X $1airlines.arff SRP 500 0 0 99\n",
      "X $1airlines.arff SRP 500 0 0 179\n",
      "X $1airlines.arff OBagAd 500 0 0 56\n",
      "X $1airlines.arff OBagAd 500 0 0 281\n",
      "X $1airlines.arff OBagAd 500 0 0 506\n",
      "X $1airlines.arff OBagASHT 500 0 0 146\n",
      "X $1airlines.arff OBagASHT 500 0 0 731\n",
      "X $1airlines.arff OBagASHT 500 0 0 1317\n",
      "X $1airlines.arff OBag 500 0 0 175\n",
      "X $1airlines.arff OBag 500 0 0 875\n",
      "X $1airlines.arff OBag 500 0 0 1575\n",
      "X $1elecNormNew.arff ARF 500 0 0 85\n",
      "X $1elecNormNew.arff ARF 500 0 0 428\n",
      "X $1elecNormNew.arff ARF 500 0 0 770\n",
      "X $1elecNormNew.arff LBag 500 0 0 143\n",
      "X $1elecNormNew.arff LBag 500 0 0 716\n",
      "X $1elecNormNew.arff LBag 500 0 0 1290\n",
      "X $1elecNormNew.arff SRP 500 0 0 40\n",
      "X $1elecNormNew.arff SRP 500 0 0 203\n",
      "X $1elecNormNew.arff SRP 500 0 0 365\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 256\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 1282\n",
      "X $1elecNormNew.arff OBagAd 500 0 0 2308\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 303\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 1515\n",
      "X $1elecNormNew.arff OBagASHT 500 0 0 2727\n",
      "X $1elecNormNew.arff OBag 500 0 0 309\n",
      "X $1elecNormNew.arff OBag 500 0 0 1549\n",
      "X $1elecNormNew.arff OBag 500 0 0 2788\n",
      "X $1GMSC.arff ARF 500 0 0 140\n",
      "X $1GMSC.arff ARF 500 0 0 704\n",
      "X $1GMSC.arff ARF 500 0 0 1268\n",
      "X $1GMSC.arff LBag 500 0 0 184\n",
      "X $1GMSC.arff LBag 500 0 0 922\n",
      "X $1GMSC.arff LBag 500 0 0 1661\n",
      "X $1GMSC.arff SRP 500 0 0 72\n",
      "X $1GMSC.arff SRP 500 0 0 364\n",
      "X $1GMSC.arff SRP 500 0 0 655\n",
      "X $1GMSC.arff OBagAd 500 0 0 442\n",
      "X $1GMSC.arff OBagAd 500 0 0 2211\n",
      "X $1GMSC.arff OBagAd 500 0 0 3980\n",
      "X $1GMSC.arff OBagASHT 500 0 0 520\n",
      "X $1GMSC.arff OBagASHT 500 0 0 2604\n",
      "X $1GMSC.arff OBagASHT 500 0 0 4687\n",
      "X $1GMSC.arff OBag 500 0 0 485\n",
      "X $1GMSC.arff OBag 500 0 0 2425\n",
      "X $1GMSC.arff OBag 500 0 0 4365\n",
      "X $1covtypeNorm.arff ARF 500 0 0 86\n",
      "X $1covtypeNorm.arff ARF 500 0 0 432\n",
      "X $1covtypeNorm.arff ARF 500 0 0 777\n",
      "X $1covtypeNorm.arff LBag 500 0 0 55\n",
      "X $1covtypeNorm.arff LBag 500 0 0 279\n",
      "X $1covtypeNorm.arff LBag 500 0 0 503\n",
      "X $1covtypeNorm.arff SRP 500 0 0 16\n",
      "X $1covtypeNorm.arff SRP 500 0 0 83\n",
      "X $1covtypeNorm.arff SRP 500 0 0 149\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 96\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 484\n",
      "X $1covtypeNorm.arff OBagAd 500 0 0 872\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 101\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 508\n",
      "X $1covtypeNorm.arff OBagASHT 500 0 0 915\n",
      "X $1covtypeNorm.arff OBag 500 0 0 103\n",
      "X $1covtypeNorm.arff OBag 500 0 0 515\n",
      "X $1covtypeNorm.arff OBag 500 0 0 928\n",
      "X $1kyoto_binary.arff ARF 500 0 0 159\n",
      "X $1kyoto_binary.arff ARF 500 0 0 799\n",
      "X $1kyoto_binary.arff ARF 500 0 0 1439\n",
      "X $1kyoto_binary.arff LBag 500 0 0 211\n",
      "X $1kyoto_binary.arff LBag 500 0 0 1059\n",
      "X $1kyoto_binary.arff LBag 500 0 0 1906\n",
      "X $1kyoto_binary.arff SRP 500 0 0 74\n",
      "X $1kyoto_binary.arff SRP 500 0 0 373\n",
      "X $1kyoto_binary.arff SRP 500 0 0 673\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 445\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 2227\n",
      "X $1kyoto_binary.arff OBagAd 500 0 0 4009\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 455\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 2279\n",
      "X $1kyoto_binary.arff OBagASHT 500 0 0 4102\n",
      "X $1kyoto_binary.arff OBag 500 0 0 437\n",
      "X $1kyoto_binary.arff OBag 500 0 0 2187\n",
      "X $1kyoto_binary.arff OBag 500 0 0 3936\n"
     ]
    }
   ],
   "source": [
    "path_file='../scripts/data-pi-rates'\n",
    "parse_folder_to_file('energy_pi/get_rates', f'{path_file}')\n",
    "# parse_folder_to_file('acc-small', '../scripts/data-acc')\n",
    "\n",
    "df = load_df(f'{path_file}.csv')\n",
    "# df\n",
    "df['IPS'] = df['instances'] / df['time']\n",
    "\n",
    "esize = 25\n",
    "incre = False\n",
    "for bsize in [50, 250, 500]:\n",
    "    print(f\"\\n\\n\\n--------------------\\nesize {esize}\\nbsize {bsize}\")\n",
    "    calculate_rate_bsize(esize, bsize, incre)\n",
    "    incre = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_rate_bsize(100, 250, incremental_included=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_rate_bsize(100, 500, incremental_included=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "## Energy processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOA logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_MOA(fname):\n",
    "    global header_printed\n",
    "    columns = []\n",
    "    wanted = ['learning evaluation instances', 'Wall Time (Actual Time)', 'Avg Delay (ms)', 'outRate (inst/s)']\n",
    "    pstr = ''\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    for s in spname[1:]:\n",
    "        pstr += s + ','\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                spline = line.split(',')\n",
    "                for s in spline:\n",
    "                    if s in wanted:\n",
    "                        columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        for c in columns:\n",
    "            pstr += spline[c] + ','\n",
    "        if len(columns) == 2:\n",
    "            pstr += '1,'\n",
    "        if not header_printed:\n",
    "            head = 'dataset,algorithm,ensemble_size,cores,batch_size,inc_rate,instances,time,delay,out_rate'\n",
    "            pstr = f\"{head}\\n{pstr}\"\n",
    "            header_printed = True\n",
    "        return (pstr[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MOA(folder, out_file):\n",
    "    directory = os.fsencode(folder)\n",
    "    global header_printed\n",
    "\n",
    "    with open(out_file, \"w+\") as output:\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.startswith(\"term-\"): \n",
    "                output.write(f\"{parse_MOA(f'{os.fsdecode(directory)}/{filename}')}\\n\")\n",
    "    fname = os.fsdecode(out_file)\n",
    "    df = pd.read_csv(fname)\n",
    "    df['inc_rate'].astype('int64')\n",
    "    return df[['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size',\n",
    "               'instances', 'time', 'delay', 'out_rate']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Wmeas(filename):\n",
    "    return pd.read_csv(filename, header=None, names=['date', 'time', 'measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exper_order_to_dict(filename, d):\n",
    "    with open(filename) as forder:\n",
    "        got_timestamp = False\n",
    "        dataset = algorithm = Esize = cores = Bsize = ''\n",
    "        dnow=None\n",
    "        for line in forder:\n",
    "            if not got_timestamp:\n",
    "                spline = [i.strip() for i in line.split(' ')]\n",
    "                sdate,stime = spline\n",
    "                date_time_obj = datetime.datetime.strptime(f'{sdate} {stime}', '%d/%m/%y %H:%M:%S')\n",
    "                got_timestamp = True\n",
    "                if dnow:\n",
    "                    dnow['finish'] = date_time_obj - datetime.timedelta(seconds=1)\n",
    "            elif ':' not in line:\n",
    "                spline = line.split('/')[-1].strip().split('-')\n",
    "#                 print(spline)\n",
    "                if len(spline) == 6:\n",
    "                    dataset,algorithm,Esize,cores,Bsize,rate = spline\n",
    "                else:\n",
    "                    dataset,algorithm,Esize,cores,Bsize,rate = *spline,1\n",
    "                if algorithm not in d:\n",
    "                    d[algorithm] = {}\n",
    "#                 if method not in d[algorithm]:\n",
    "#                     d[algorithm][method] = {}\n",
    "                if dataset not in d[algorithm]:\n",
    "                    d[algorithm][dataset] = {}\n",
    "                if Esize not in d[algorithm][dataset]:\n",
    "                    d[algorithm][dataset][Esize] = {}\n",
    "                if cores not in d[algorithm][dataset][Esize]:\n",
    "                    d[algorithm][dataset][Esize][cores] = {}\n",
    "                if Bsize not in d[algorithm][dataset][Esize][cores]:\n",
    "                    d[algorithm][dataset][Esize][cores][Bsize] = {}\n",
    "                if rate not in d[algorithm][dataset][Esize][cores][Bsize]:\n",
    "                    d[algorithm][dataset][Esize][cores][Bsize][rate] = {'start': date_time_obj, 'finish': ''}\n",
    "                    dnow = d[algorithm][dataset][Esize][cores][Bsize][rate]\n",
    "                got_timestamp = False\n",
    "            else:\n",
    "                spline = [i.strip() for i in line.split(' ')]\n",
    "                sdate,stime = spline\n",
    "                date_time_obj = datetime.datetime.strptime(f'{sdate} {stime}', '%d/%m/%y %H:%M:%S')\n",
    "                got_timestamp = True\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dict(d, df):\n",
    "    df['datetime'] = df['date'] + ' ' + df['time']\n",
    "    for k, v in d.items():\n",
    "        for k1, v1 in v.items():\n",
    "            for k2, v2 in v1.items():\n",
    "                for k3, v3 in v2.items():\n",
    "                    for k4, v4 in v3.items():\n",
    "                        for k5, v5 in v4.items():\n",
    "#                             for k6, v6 in v5.items():\n",
    "                            if 'seconds' not in v5:\n",
    "                                st = v5['start']\n",
    "                                ed = v5['finish']\n",
    "                                v5['seconds'] = (ed - st).seconds\n",
    "                                new_df = df[(df.datetime <= f'{ed.strftime(\"%d/%m/%y\")} {ed.strftime(\"%X\")}')\n",
    "                                          & (df.datetime >= f'{st.strftime(\"%d/%m/%y\")} {st.strftime(\"%X\")}')]\n",
    "                                v5['avg_measure'] = new_df['measure'].mean()\n",
    "                                v5['sum_measure'] = new_df['measure'].sum()\n",
    "                                v5['avg_times_seconds'] = v5['avg_measure'] * v5['seconds']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_to_df(d, ensemble_size=False):\n",
    "    dappend = {'algorithm': [], 'dataset': [], 'ensemble_size': [], 'cores': [], \n",
    "               'batch_size': [], 'duration': [], 'inc_rate': [], 'avg_measure': [], 'sum_measure': [], 'avg_times_seconds': []}\n",
    "    for kalg,valg in d.items():\n",
    "        for kds,vds in valg.items():\n",
    "            for kens,vens in vds.items():\n",
    "                for kcore,vcore in vens.items():\n",
    "                    for kmbs,vmbs in vcore.items():\n",
    "                        for krate,vrate in vmbs.items():\n",
    "                            dappend['algorithm'].append(kalg)\n",
    "                            dappend['dataset'].append(kds)\n",
    "                            dappend['ensemble_size'].append(kens)\n",
    "                            dappend['cores'].append(kcore)\n",
    "                            dappend['batch_size'].append(kmbs)\n",
    "                            dappend['duration'].append(vrate['seconds'])\n",
    "                            dappend['inc_rate'].append(krate)\n",
    "                            for key in ['avg_measure', 'sum_measure','avg_times_seconds']:\n",
    "                                dappend[key].append(vrate[key])\n",
    "    adf = pd.DataFrame(data=dappend)\n",
    "    adf = adf.sort_values(['algorithm','dataset']).astype({'inc_rate': 'int64', \n",
    "                                                           'cores': 'int64',\n",
    "                                                           'batch_size': 'int64'})\n",
    "    if ensemble_size:\n",
    "        return adf[['algorithm', 'dataset', 'ensemble_size', 'inc_rate', 'cores', 'batch_size',\n",
    "               'duration', 'avg_measure', 'sum_measure']]\n",
    "    return adf[['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size',\n",
    "               'duration', 'avg_measure', 'sum_measure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse SSH logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_SSH(fname):\n",
    "    fname = os.fsdecode(fname)\n",
    "    read_ssh = False\n",
    "    alg = ''\n",
    "    dataset = ''\n",
    "    rate = ''\n",
    "    d = {'algorithm': [], 'dataset': [], 'inc_rate': [], 'prod_rate': [], 'tt_inst_prod': []}\n",
    "    with open (fname, \"r\") as file:\n",
    "        for line in file:\n",
    "            if not read_ssh:\n",
    "                if 'ssh-' in line:\n",
    "                    read_ssh = True\n",
    "                    dataset, alg, rate = line.split('-')[1:]\n",
    "                    d['algorithm'].append(alg)\n",
    "                    d['dataset'].append(dataset)\n",
    "                    d['inc_rate'].append(rate.strip())\n",
    "            else:\n",
    "                if 'Total instances Producer' in line:\n",
    "                    tt_inst = float(line.split(': ')[1])\n",
    "                    d['tt_inst_prod'].append(tt_inst)\n",
    "                elif 'Producer Rate' in line:\n",
    "                    prod_rate = float(line.split(': ')[1])\n",
    "                    d['prod_rate'].append(prod_rate)\n",
    "                    read_ssh = False\n",
    "    return pd.DataFrame.from_dict(d).astype({'inc_rate': 'int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN MOTHERFUCKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "%cd pi\n",
    "d = {}\n",
    "df = load_Wmeas(f'energy/Wmeasure.log')\n",
    "exper_order_to_dict(f'energy/exper_order.log')\n",
    "d = populate_dict(d)\n",
    "adf = append_dict_to_df(d, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "header_printed = False\n",
    "moaDF = read_MOA(\"vostro/energy/energy-vostro\", \"vostro/energy/inst-and-delay.csv\")\n",
    "measureDF = load_Wmeas(f'vostro/energy/Wmeasure_vostro.log')\n",
    "d = exper_order_to_dict(f'vostro/energy/exper_order_vostro.log', d)\n",
    "d = populate_dict(d, measureDF)\n",
    "mdf = append_dict_to_df(d)\n",
    "sshDF = parse_SSH(f'vostro/energy/energy-vostro/ssh-log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join dfs\n",
    "finaldf = moaDF.merge(mdf, on=['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size'])\n",
    "finaldf = finaldf.merge(sshDF, on=['algorithm', 'dataset', 'inc_rate'])\n",
    "finaldf['joules'] = finaldf['avg_measure'] * finaldf['time']\n",
    "finaldf['JPI'] = finaldf['joules'] / finaldf['instances']\n",
    "# finaldf['JP1kI'] = finaldf['joules'] / (finaldf['instances']/1000)\n",
    "# finaldf['JPIoriginal'] = finaldf['JPI']\n",
    "# finaldf['JPI'] = finaldf['JP1kI']\n",
    "# finaldf.dataset.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PERC column to identify if it used 90, 50 or 10% max rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = finaldf.sort_values(by=['dataset','algorithm','inc_rate'], ascending=False)\n",
    "tdf['PERC'] = 0\n",
    "masks = {'90':[], '50': [], '10': []}\n",
    "for k, v in zip(masks.keys(), [0, 1, 2]):\n",
    "#         x = 1 if i % 3 == v else 0\n",
    "    for i in range(len(tdf)):\n",
    "        masks[k].append(i % 3 == v)\n",
    "tdf.loc[masks['90'],'PERC'] = '90'\n",
    "tdf.loc[masks['50'],'PERC'] = '50'\n",
    "tdf.loc[masks['10'],'PERC'] = '10'\n",
    "# tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show DFs for each algorithm and dataset, divided by rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = ['Ada', 'L', 'Patch', 'Adwin', 'ASHT', 'OzaBag']\n",
    "datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm']\n",
    "for k in ['90', '50', '10']:\n",
    "#     print(f\"\\n\\n\\n\\n{k}\")\n",
    "    energy = tdf[tdf.PERC == k]\n",
    "    for ds in energy.dataset.unique():\n",
    "        for alg in algs:\n",
    "#             auxdf = energy[(energy.dataset == ds) & (energy.algorithm.str.contains(alg))]\n",
    "            auxdf = filter_by_substring_algorithm(energy[energy.dataset == ds], alg)\n",
    "#             display(auxdf)\n",
    "#             if alg == 'Patch' or alg == 'Ada':\n",
    "#                 display(auxdf[['algorithm','dataset','cores','batch_size','prod_rate','out_rate','instances','time','joules','JPI', 'JP1kI']].sort_values(['cores','batch_size']))\n",
    "#         auxdf = energy[(energy.dataset == ds) & (~energy.algorithm.str.contains('|'.join(algs)))]\n",
    "#         display(auxdf[['algorithm','dataset','cores','batch_size','prod_rate','out_rate','instances','time','joules','JPI', 'JP1kI']].sort_values(['cores','batch_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing and preparing for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fields(df):\n",
    "    wanted = ['algorithm', 'dataset', 'batch_size', 'cores', 'out_rate', 'instances', 'delay', 'joules', 'JPI']\n",
    "    return df[[\n",
    "        l for l in df.columns if any([w in l for w in wanted])\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_by_rate(df):\n",
    "#     display(df)\n",
    "    rate = df.PERC.iloc[0]\n",
    "    return df.rename(columns={\"out_rate\": f\"out_rate_{rate}\", \"instances\": f\"instances_{rate}\",\n",
    "                              \"delay\": f\"delay_{rate}\", \"joules\": f\"joules_{rate}\", \"JPI\": f\"JPI_{rate}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_norm(df, x='90MB'):\n",
    "    mdf = df.iloc[:,[0,1,15,30,45]]\n",
    "    if x == '90MB':\n",
    "        thisisone = mdf.iloc[2,4]\n",
    "#     else:\n",
    "    elif x == '10S':\n",
    "        thisisone = mdf.iloc[0,2]\n",
    "    else:\n",
    "#         10P\n",
    "        thisisone = mdf.iloc[1,2]\n",
    "    for i in ['JPI_10','JPI_50','JPI_90']:\n",
    "           df[i] = df[i]/thisisone\n",
    "#     display(df.iloc[:,[0,1,15,30,45]])\n",
    "    return df.iloc[:,[0,1,15,30,45,7,22,37]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linhas pretas (solida, tracejada, pontilhada)\n",
    "\n",
    "JPI em barras (3 barras por rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_JPI_delay(df, ax, mJPI, mDel, legend=False, title=False, ylabels=False, ds='', bar=False, share_y=False, log_y=False, norm=False, hide=True):\n",
    "    if norm:\n",
    "        df = my_norm(df, x=norm)\n",
    "    global rate\n",
    "    global twin\n",
    "    width = 0.20\n",
    "    alg_order = ['Sequential', 'B1', 'B500']\n",
    "    labels = ['10%', '50%', '90%']\n",
    "    line_format = ['-', '--', ':']\n",
    "    linfo = '--'\n",
    "    x = np.arange(len(labels))\n",
    "    lns_l = []\n",
    "    for i in range(3):\n",
    "        adf = df.iloc[[i]]\n",
    "        values_j = [adf.JPI_10.iloc[0], adf.JPI_50.iloc[0], adf.JPI_90.iloc[0]]\n",
    "        if bar:\n",
    "            lns_l += ax.bar(x - ((1 - i) * width), values_j, width, label=f'JPI-{alg_order[i]}')            \n",
    "        else:\n",
    "            lns_l += ax.plot(x, values_j, label=f'JPI-{alg_order[i]}')\n",
    "            \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax_r = ax.twinx()\n",
    "    twin = ax_r\n",
    "    if log_y:\n",
    "        ax.set_yscale('log')\n",
    "        ax_r.set_yscale('log')\n",
    "    \n",
    "    if title:\n",
    "        alg_title = re.sub('Sequential', '', df.algorithm.iloc[0])\n",
    "        ax.set_title(f'{alg_title}')\n",
    "#         ax.set_xlabel('Rate')\n",
    "    \n",
    "    if ylabels:\n",
    "        ax.set_ylabel(ds)\n",
    "#         ax.set_ylabel('JPI')\n",
    "#         ax_r.set_ylabel('delay')\n",
    "        \n",
    "    if last:\n",
    "        ax.set_ymargin(2)\n",
    "    \n",
    "    for i in range(3):\n",
    "        adf = df.iloc[[i]]\n",
    "        values_d = [ x/1000 for x in [adf.delay_10.iloc[0], adf.delay_50.iloc[0], adf.delay_90.iloc[0]]]\n",
    "        if bar:\n",
    "            linfo = f'k{line_format[i]}'\n",
    "        lns_l += ax_r.plot(x, values_d, linfo, label=f'delay-{alg_order[i]}')\n",
    "    labs = [l.get_label() for l in lns_l]\n",
    "    if legend:\n",
    "        ax.legend(lns_l, labs, loc=0)\n",
    "    if hide:\n",
    "        ax_r.set_yticklabels([])\n",
    "    if share_y == 'row':\n",
    "        ax.set_ylim(top=mJPI)\n",
    "        ax_r.set_ylim(top=mDel/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_graphNx1(ds, axis, id_ds, df, bar=False, share_y='row', log_y=False, norm=False):\n",
    "#     print(f'aux {bar}')\n",
    "    rates = ['10', '50', '90']\n",
    "    algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "    global title\n",
    "    global labels\n",
    "    hide_axis = True\n",
    "#     fig.suptitle(f'JPI and delay for {ds}', fontsize=18, y=1)\n",
    "#     get max value from delay for all rates on all algorithms for this dataset\n",
    "    mLstJPI = []\n",
    "    mLstDel = []\n",
    "    if share_y == 'row':\n",
    "        for rt in rates:\n",
    "            rtDF = df[df.PERC == rt]\n",
    "            mLstJPI.append(rtDF.JPI.max())\n",
    "            mLstDel.append(rtDF.delay.max())\n",
    "        mJPI = max(mLstJPI)*1.05\n",
    "        mDel = max(mLstDel)*1.05\n",
    "    else:\n",
    "        mJPI = mDel = 0\n",
    "    for alg in algs:\n",
    "        dsalgdf = filter_by_substring_algorithm(df, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "        for rt in rates:\n",
    "            if rt == '10':\n",
    "#                 display(dsalgdf)\n",
    "                showdf = rename_columns_by_rate(dsalgdf[dsalgdf.PERC == rt])\n",
    "            else:\n",
    "                to_join = dsalgdf[dsalgdf.PERC == rt]\n",
    "                showdf = showdf.merge(rename_columns_by_rate(to_join),\n",
    "                                  on=['algorithm', 'dataset', 'batch_size', 'cores']).sort_values(['batch_size','cores'])\n",
    "        if 'Ada' in alg:\n",
    "            show_graph_JPI_delay(showdf, axis[id_ds][algs.index(alg)], mJPI, mDel, title=title, ylabels=True, ds=ds, bar=bar, share_y=share_y, log_y=log_y, norm=norm)\n",
    "        else:\n",
    "            if algs[-1] == alg:\n",
    "                hide_axis = False\n",
    "            show_graph_JPI_delay(showdf, axis[id_ds][algs.index(alg)], mJPI, mDel, title=title, bar=bar, share_y=share_y, log_y=log_y, norm=norm, hide=hide_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(bars=False, share_y='row', log_y=False, norm=False):\n",
    "#     print(f'gen {bars}')\n",
    "    datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm', 'kyoto_binary']\n",
    "    print(share_y)\n",
    "    fig, axis = plt.subplots(len(datasets), 6, figsize=(12,9), tight_layout=True, sharey=share_y)\n",
    "    global title\n",
    "    global labels\n",
    "    global last\n",
    "    global twin\n",
    "    leg = False\n",
    "    labls = True\n",
    "    title = True\n",
    "    last = False\n",
    "    twin = axis[0][0]\n",
    "    for ds in datasets:\n",
    "        if datasets.index(ds) == (len(datasets) - 1):\n",
    "            last = True\n",
    "        dsdf = tdf[tdf.dataset == ds]\n",
    "#         display(dsdf)\n",
    "        aux_graphNx1(ds, axis, datasets.index(ds), dsdf, bar=bars, share_y=share_y, log_y=log_y, norm=norm)\n",
    "        title = False\n",
    "    lines_1, labels_1 = axis[0][0].get_legend_handles_labels()\n",
    "    lines_2, labels_2 = twin.get_legend_handles_labels()\n",
    "    lines = lines_1 + lines_2\n",
    "    labels = labels_1 + labels_2\n",
    "    lgd = fig.legend(lines, labels, loc=8, ncol=6, bbox_to_anchor=(0.5, -0.02))\n",
    "    lgd.set_in_layout(True)\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    filename = 'bars-all-4x1-JPI-delay.eps' if bars else 'all-4x1-JPI-delay.eps'\n",
    "    plt.savefig(f'Vostro-{filename}', pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = ['10', '50', '90']\n",
    "algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm','kyoto_binary']\n",
    "all_values = []\n",
    "# filtra algoritmo\n",
    "for alg in algs:\n",
    "#         line = alg + ' & $\\Delta$ '\n",
    "    line = alg + '  '\n",
    "    algdf = filter_by_substring_algorithm(tdf, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "    # filtra dataset\n",
    "    for ds in datasets:\n",
    "        dsalgdf = algdf[algdf.dataset == ds]\n",
    "#         display(dsalgdf.head())\n",
    "        # \"junta\"\n",
    "        for rt in rates:\n",
    "            if rt == '10':\n",
    "                showdf = rename_columns_by_rate(dsalgdf[dsalgdf.PERC == rt])\n",
    "            else:\n",
    "                to_join = dsalgdf[dsalgdf.PERC == rt]\n",
    "                showdf = showdf.merge(rename_columns_by_rate(to_join),on=['algorithm', 'dataset', 'batch_size', 'cores']).sort_values(['batch_size','cores'])\n",
    "#         showdf = showdf[['algorithm','dataset','cores','batch_size','JPI_10','JPI_50','JPI_90']]\n",
    "#         display(showdf)\n",
    "        for i in ['10', '50', '90']:\n",
    "            minoutro = min(showdf[showdf.batch_size == 1][f'JPI_{i}'])\n",
    "#             print(f\"JPI_{i} minoutro {minoutro}\")\n",
    "            val = ((showdf[showdf.batch_size != 1][f'JPI_{i}'].iloc[0] - minoutro) / minoutro ) * 100\n",
    "            all_values.append(val)\n",
    "            sval = f\"{val:.2f} \" if val < 0 else \"\\\\textbf{ \" + f\"{val:.2f}\" + \"} \"\n",
    "            line += f\"& {sval}\"\n",
    "    print(f\"{line} \\\\\\\\\")\n",
    "print(f'\\n\\nAverage reduction: {sum(all_values)/len(all_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_delta_rates_vert(ratio=False):\n",
    "    rates = ['10', '50', '90']\n",
    "    algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "    datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm','kyoto_binary']\n",
    "    all_values = []\n",
    "    # filtra algoritmo\n",
    "    for alg in algs:\n",
    "        line = '\\hline\\n\\\\multirow{3}{*}{' + alg + '} '\n",
    "        algdf = filter_by_substring_algorithm(tdf, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "#         display(algdf)\n",
    "        # filtra rate\n",
    "        for rt in rates:\n",
    "            if rt != '10':\n",
    "                line += '\\\\\\\\\\n'\n",
    "            line += f' & {rt} '\n",
    "            rtalgdf = rename_columns_by_rate(algdf[algdf.PERC == rt])\n",
    "            # filtra dataset\n",
    "            for ds in datasets:\n",
    "#                 print(ds)\n",
    "                dsrtalgdf = rtalgdf[rtalgdf.dataset == ds]\n",
    "#                 display(dsrtalgdf)\n",
    "                minoutro = min(dsrtalgdf[dsrtalgdf.batch_size == 1][f'JPI_{rt}'])\n",
    "#                 print(f\"JPI_{rt} minoutro {minoutro}\")\n",
    "                val = ((dsrtalgdf[dsrtalgdf.batch_size != 1][f'JPI_{rt}'].iloc[0] - minoutro)/minoutro)*100 if ratio else dsrtalgdf[dsrtalgdf.batch_size != 1][f'JPI_{rt}'].iloc[0] - minoutro \n",
    "                all_values.append(val)\n",
    "                sval = f\"{val:.2f} \" if val < 0 else \"\\\\textbf{ \" + f\"{val:.2f}\" + \"} \"\n",
    "                line += f\"& {sval} \"\n",
    "        print(f\"{line} \\\\\\\\\")\n",
    "    print(f'\\n\\nAverage reduction: {sum(all_values)/len(all_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_delta_rates_vert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharey = row\n",
    "\n",
    "Linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = False\n",
    "\n",
    "linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "log scale y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "normalizado MB 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='90MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "normalizado Seq10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='10S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='10P')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
