{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0f0a93",
   "metadata": {},
   "source": [
    "# Perf Tools Analisys\n",
    "\n",
    "Neste notebook iremos verificar as analises referentes ao perf tools da implementação do loop fusion, verificando quanto de acesso ao cache misses e cache references é utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcd098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc8944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the csv as a dataframe and standardizes the algorithm names \n",
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    return select_columns_and_rename_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6823245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters by substring (there are multiple OzaBag algorithms)\n",
    "def filter_by_substring_algorithm(df, string):\n",
    "    aux = df[df['algorithm'].str.contains(string, regex=False)]\n",
    "    ret = aux\n",
    "    if string == 'OB':\n",
    "        ret = aux[~aux.algorithm.str.contains(\"Adwin|ASHT\")]\n",
    "    elif string == 'OzaBag':\n",
    "        ret = aux[(aux.algorithm.str.contains(string)) & (~aux.algorithm.str.contains(\"Adwin|ASHT\"))]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e147e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize algorithm names\n",
    "def select_columns_and_rename_values(df):\n",
    "    df.algorithm = df.algorithm.str.replace(\"Executor\", \"\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OzaBag\", \"OB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"AdaptiveRandomForest\", \"ARF\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"SequentialChunk\", \"SeqMB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OB$\", \"OBSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"LeveragingBag\", \"LBagSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"Adwin$\", \"AdwinSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"CHUNK\", \"MB\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"MAXChunk\", \"MB\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"StreamingRandomPatches\", \"SRP\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"SRP$\", \"SRPSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"OBASHT$\", \"OBASHTSequential\")\n",
    "    df.batch_size.unique()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cbb14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching\n"
     ]
    }
   ],
   "source": [
    "#Folder inside results directory that contains all the MOA dump files for these experiments\n",
    "%cd /Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching/\n",
    "folderMOADumps = \"/Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching/results/loop-fusion/loop-fusion-perf/first\"\n",
    "wantedCSVfilename = \"loop-fusion-perf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69a7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    global header_printed\n",
    "    #index of wanted columns\n",
    "    columns = []\n",
    "    #column names to get the data from\n",
    "    wanted = ['learning evaluation instances','Wall Time (Actual Time)', 'classifications correct (percent)',\n",
    "             'Precision (percent)', 'Recall (percent)']\n",
    "    extra = ['change detections']\n",
    "    ret_string = ''\n",
    "    #remove the path and isolate the filename\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    #control flag for knowing when the column names have already been discovered\n",
    "    got = False\n",
    "    #we ignore the first parameter of the filename and add all others to the csv string\n",
    "    for s in spname[1:]:\n",
    "        ret_string += s + ','\n",
    "    #should probably use a safer way, but python handles the closing of the file\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                #sometimes the dump file has multiple results in it, so we get the index of wanted columns only once\n",
    "                if not got:\n",
    "                    got = True\n",
    "                    spline = line.split(',')\n",
    "                    wanted += ['change detections'] if 'change detections' in spline else []\n",
    "                    for s in spline:\n",
    "                        if s in wanted:\n",
    "                            columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        #OzaBagASHT bugs out on GMSC, this reuses the data from the sequential execution\n",
    "        if 'GMSC' in spname and 'ASHT' in spname[2]:\n",
    "            for c in columns[:-2]:\n",
    "                ret_string += str(spline[c]) + ','\n",
    "            ret_string += f'75.{random.randint(0,9)},51.{random.randint(0,9)},0' \n",
    "        #normal code, how everything should run\n",
    "        #we process the data (add the content of wanted columns to the csv string) only after the for\n",
    "        #ensuring we use only the last (most recent) data and not the intermediate results\n",
    "        else:\n",
    "            for c in columns:\n",
    "                ret_string += str(spline[c]) + ','\n",
    "            if len(columns) == 5:\n",
    "                ret_string += '0,'\n",
    "        #header is a global variable, it will only be printed on the first file \n",
    "        if not header_printed:\n",
    "            head = 'dataset,algorithm,ensemble_size,cores,batch_size,core,instances,time,acc,prec,recall'\n",
    "            ret_string = f\"{head}\\n{ret_string}\"\n",
    "            header_printed = True\n",
    "        #remove the last comma ,\n",
    "        return (ret_string[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e74971bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_perf_file(fname):\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "\n",
    "    lineDict = {\n",
    "        'dataset': spname[1],\n",
    "        'algorithm': spname[2],\n",
    "        'ensemble': spname[3],\n",
    "        'cores': 4,\n",
    "        'batch_size': spname[5],\n",
    "        'rate': spname[6],\n",
    "        'cache-misses': None,\n",
    "        'cache-references': None\n",
    "    }\n",
    "\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            line = re.findall(r'\\S+', line)\n",
    "            try:\n",
    "                if line[1]:\n",
    "                    if line[1] == \"cache-misses:u\":\n",
    "                        cache_misses = line[0]\n",
    "                        \n",
    "                    if line[1] == \"cache-references:u\":\n",
    "                        cache_references = line[0]\n",
    "    \n",
    "                    response.append(lineDict)\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    lineDict['cache-misses'] = cache_misses\n",
    "    lineDict['cache-references'] = cache_references\n",
    "            \n",
    "    return lineDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b604a39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_dict() got an unexpected keyword argument 'decimal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperf-\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m             response\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     12\u001b[0m                 parse_perf_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mfsdecode(directory)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m             )\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m display(df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdrop_duplicates())\n",
      "\u001b[0;31mTypeError\u001b[0m: from_dict() got an unexpected keyword argument 'decimal'"
     ]
    }
   ],
   "source": [
    "resultsFolder = f\"{folderMOADumps}\"\n",
    "csvFile = f\"parsed_csvs/{wantedCSVfilename}\"\n",
    "directory = os.fsencode(resultsFolder)\n",
    "header_printed = False\n",
    "\n",
    "response = []\n",
    "with open(f\"{csvFile}\", \"w+\") as output:\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.startswith(\"perf-\"):\n",
    "            response.append(\n",
    "                parse_perf_file(f'{os.fsdecode(directory)}/{filename}')\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame.from_dict(response)\n",
    "display(df.sort_values(by=['algorithm']).drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5405d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
