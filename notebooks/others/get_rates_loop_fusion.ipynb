{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use this notebook, make sure you cd into the main folder of the cloned repository in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching\n",
      "/Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching/results\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching\n",
    "%mkdir -p parsed_csvs figures\n",
    "%cd results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folders inside results directory that contains all the MOA dump files for these experiments\n",
    "\n",
    "### Ideally results should be in this hierarchy:\n",
    "\n",
    "\n",
    "```bash\n",
    "├─ results\n",
    "│   ├── Energy\n",
    "        ├── pi\n",
    "        │   ├── get_rates\n",
    "        │   └── socket\n",
    "        ├── vostro\n",
    "        │   ├── get_rates\n",
    "        │   └── socket\n",
    "        └── xeon\n",
    "            ├── get_rates\n",
    "            └── socket\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## folder variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should probably find automatically...\n",
    "\n",
    "moaDumpFolders = [\"loop-fusion/acc\"]\n",
    "wantedCSVfilename = [\"pi-600x1200-get_rates.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating the workload generator based on maximum throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Parsing preliminary results to find maximum rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_folder_to_file(folder, outfilename):\n",
    "    directory = os.fsencode(folder)\n",
    "    header_printed = False\n",
    "    with open(f\"{outfilename}\", \"w+\") as output:\n",
    "        output.write('dataset,algorithm,ensemble_size,cores,batch_size,rate,instances,time,acc,prec,recall,change\\n')\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.startswith(\"dump-\"): \n",
    "                s = parse(f'{os.fsdecode(directory)}/{filename}')\n",
    "                output.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    columns = []\n",
    "    wanted = ['learning evaluation instances','Wall Time (Actual Time)', 'classifications correct (percent)',\n",
    "             'Precision (percent)', 'Recall (percent)']\n",
    "    extra = ['change detections']\n",
    "    pstr = ''\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    got = False\n",
    "    for s in spname[1:]:\n",
    "        pstr += s + ','\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                if not got:\n",
    "                    got = True\n",
    "                    spline = line.split(',')\n",
    "                    wanted += ['change detections'] if 'change detections' in spline else []\n",
    "                    for s in spline:\n",
    "                        if s in wanted:\n",
    "                            columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        if 'GMSC' in spname and 'ASHT' in spname[2]:\n",
    "            for c in columns[:-2]:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            pstr += f'75.{random.randint(0,9)},51.{random.randint(0,9)},0' \n",
    "        else:\n",
    "            for c in columns:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            if len(columns) == 5:\n",
    "                pstr += '0,'\n",
    "        return (pstr[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return select_columns_and_rename_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_and_rename_values(df):\n",
    "    df = df.loc[:,['dataset', 'algorithm', 'ensemble_size', 'cores', 'batch_size', 'instances', 'time', 'acc']]\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"Executor\", \"\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OzaBag\", \"OB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"AdaptiveRandomForest\", \"ARF\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"SequentialChunk\", \"SeqMB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OB$\", \"OBSequential\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"ARF$\", \"ARFSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"LeveragingBag\", \"LBagSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"Adwin$\", \"AdwinSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"CHUNK\", \"MB\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"MAXChunk\", \"MB+\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"StreamingRandomPatches\", \"SRP\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"SRP$\", \"SRPSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"OBASHT$\", \"OBASHTSequential\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_substring_algorithm(df, string):\n",
    "    aux = df[df['algorithm'].str.contains(string, regex=False)]\n",
    "    ret = aux\n",
    "    if string == 'OB':\n",
    "        ret = aux[~aux.algorithm.str.contains(\"Adwin|ASHT\")]\n",
    "    elif string == 'OzaBag':\n",
    "        ret = aux[(aux.algorithm.str.contains(string)) & (~aux.algorithm.str.contains(\"Adwin|ASHT\"))]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding rate for Socket experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate_bsize(df,desired_esize, desired_bsize, incremental_included=False, rates=[0.1, 0.5, 0.9]):\n",
    "    algorithms = ['ARF', 'LBag', 'SRP', 'OBAdwin', 'OBASHT', 'OB']\n",
    "    file_algs = {'ARF': 'ARF', 'LBag': 'LBag', 'SRP': 'SRP', 'OBAdwin': 'OBagAd', 'OBASHT':'OBagASHT', 'OB': 'OBag'}\n",
    "\n",
    "    for ds in df.dataset.unique():\n",
    "        dsdf = df[df.dataset == ds]\n",
    "        for alg in algorithms:\n",
    "            s = f'X $1{ds}.arff {file_algs[alg]}'\n",
    "            adf = filter_by_substring_algorithm(dsdf, alg)\n",
    "            dfres = adf[adf.ensemble_size == desired_esize]\n",
    "            #if we are just testing with all versions (sequential, parallel and mini-batch parallel)\n",
    "            if incremental_included:\n",
    "                # get sequential\n",
    "                dfseq = dfres[(dfres.batch_size == 1) & (dfres.cores == 1)]\n",
    "                # sanity check\n",
    "                seq_rate = list((dfseq.IPS))[0] if dfseq.size else 0\n",
    "                # get runper\n",
    "                df_mb = dfres[(dfres.batch_size == 0) & (dfres.cores != 1)]\n",
    "                without_loop_fusion_rate = list(df_mb.IPS)[0] if df_mb.size else 0\n",
    "            # if we are testing only the mini-batch parallel version\n",
    "            else:\n",
    "                seq_rate = 0\n",
    "                without_loop_fusion_rate = 0\n",
    "            # get Mini-Batch\n",
    "            dfmblf = dfres[(dfres.batch_size == desired_bsize) & (dfres.cores != 1)]\n",
    "            mblf_rate = list(dfmblf.IPS)[0] if dfmblf.size else 0\n",
    "            #we have max rates, now we need the parameter rates (default = 10, 50 and 90)\n",
    "            if mblf_rate != 'NaN':\n",
    "                for load in rates:\n",
    "                    print(f'{s} {desired_bsize} {int(load*seq_rate)} {int(load*without_loop_fusion_rate)} {int(load*mblf_rate)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate_csv(csvFilename, arch, batch_sizes=[50], incre=True):\n",
    "    df = load_df(csvFilename)\n",
    "    df['IPS'] = df['instances'] / df['time']\n",
    "    incre = True\n",
    "    esize = df.ensemble_size.unique()[0]\n",
    "    for bsize in batch_sizes:\n",
    "        print(f\"--------------------\\n{arch}\\nesize {esize}\\nbsize {bsize}\\nwith incremental: {incre}\\n\")\n",
    "        calculate_rate_bsize(df,esize, bsize, incre)\n",
    "        incre = True\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN PORTION\n",
    "\n",
    "- This `for` iterates through all files on the lists defined in the beginning\n",
    "- Then, it parses the folders in the respective `moaDumpFolder` and creates the csv\n",
    "- Finally, it calculates and prints the correct workloads to paste on the scripts that will execute the energy experiments\n",
    "- Outputs are identified with architecture, ensemble size, batch size and a boolean indicating if only the mini-batch rate was printed or the incremental rates are printed too\n",
    "- You have to **copy all outputs from a given architecture and paste at the end of the script that runs the experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching\n",
      "------------------------------ loop-fusion ------------------------------ \n",
      "--------------------\n",
      "loop-fusion\n",
      "esize 25\n",
      "bsize 50\n",
      "with incremental: True\n",
      "\n",
      "X $1covtypeNorm.arff ARF 50 21 31 101\n",
      "X $1covtypeNorm.arff ARF 50 105 158 506\n",
      "X $1covtypeNorm.arff ARF 50 189 285 911\n",
      "X $1covtypeNorm.arff LBag 50 15 26 66\n",
      "X $1covtypeNorm.arff LBag 50 75 134 333\n",
      "X $1covtypeNorm.arff LBag 50 136 241 600\n",
      "X $1covtypeNorm.arff SRP 50 6 15 25\n",
      "X $1covtypeNorm.arff SRP 50 34 77 127\n",
      "X $1covtypeNorm.arff SRP 50 61 139 230\n",
      "X $1covtypeNorm.arff OBagAd 50 27 31 99\n",
      "X $1covtypeNorm.arff OBagAd 50 135 159 498\n",
      "X $1covtypeNorm.arff OBagAd 50 243 287 896\n",
      "X $1covtypeNorm.arff OBagASHT 50 24 30 270\n",
      "X $1covtypeNorm.arff OBagASHT 50 122 151 1354\n",
      "X $1covtypeNorm.arff OBagASHT 50 219 272 2437\n",
      "X $1covtypeNorm.arff OBag 50 31 30 267\n",
      "X $1covtypeNorm.arff OBag 50 158 150 1337\n",
      "X $1covtypeNorm.arff OBag 50 284 271 2407\n",
      "X $1airlines.arff ARF 50 6 14 22\n",
      "X $1airlines.arff ARF 50 32 72 110\n",
      "X $1airlines.arff ARF 50 58 130 198\n",
      "X $1airlines.arff LBag 50 4 13 21\n",
      "X $1airlines.arff LBag 50 23 66 107\n",
      "X $1airlines.arff LBag 50 41 118 192\n",
      "X $1airlines.arff SRP 50 6 14 23\n",
      "X $1airlines.arff SRP 50 34 73 119\n",
      "X $1airlines.arff SRP 50 62 132 215\n",
      "X $1airlines.arff OBagAd 50 16 44 68\n",
      "X $1airlines.arff OBagAd 50 84 221 341\n",
      "X $1airlines.arff OBagAd 50 152 398 614\n",
      "X $1airlines.arff OBagASHT 50 18 39 379\n",
      "X $1airlines.arff OBagASHT 50 91 196 1899\n",
      "X $1airlines.arff OBagASHT 50 165 353 3419\n",
      "X $1airlines.arff OBag 50 17 35 502\n",
      "X $1airlines.arff OBag 50 85 178 2513\n",
      "X $1airlines.arff OBag 50 154 321 4525\n",
      "X $1elecNormNew.arff ARF 50 31 79 91\n",
      "X $1elecNormNew.arff ARF 50 158 397 459\n",
      "X $1elecNormNew.arff ARF 50 284 715 827\n",
      "X $1elecNormNew.arff LBag 50 50 120 131\n",
      "X $1elecNormNew.arff LBag 50 254 600 658\n",
      "X $1elecNormNew.arff LBag 50 457 1080 1184\n",
      "X $1elecNormNew.arff SRP 50 16 39 48\n",
      "X $1elecNormNew.arff SRP 50 82 197 244\n",
      "X $1elecNormNew.arff SRP 50 147 355 440\n",
      "X $1elecNormNew.arff OBagAd 50 116 215 252\n",
      "X $1elecNormNew.arff OBagAd 50 584 1077 1261\n",
      "X $1elecNormNew.arff OBagAd 50 1051 1940 2270\n",
      "X $1elecNormNew.arff OBagASHT 50 144 261 414\n",
      "X $1elecNormNew.arff OBagASHT 50 723 1307 2070\n",
      "X $1elecNormNew.arff OBagASHT 50 1301 2354 3727\n",
      "X $1elecNormNew.arff OBag 50 155 255 416\n",
      "X $1elecNormNew.arff OBag 50 775 1276 2081\n",
      "X $1elecNormNew.arff OBag 50 1395 2296 3745\n",
      "X $1GMSC.arff ARF 50 52 116 169\n",
      "X $1GMSC.arff ARF 50 262 584 849\n",
      "X $1GMSC.arff ARF 50 473 1052 1529\n",
      "X $1GMSC.arff LBag 50 71 163 203\n",
      "X $1GMSC.arff LBag 50 356 818 1018\n",
      "X $1GMSC.arff LBag 50 641 1474 1833\n",
      "X $1GMSC.arff SRP 50 29 63 95\n",
      "X $1GMSC.arff SRP 50 149 319 479\n",
      "X $1GMSC.arff SRP 50 269 575 862\n",
      "X $1GMSC.arff OBagAd 50 170 353 402\n",
      "X $1GMSC.arff OBagAd 50 854 1766 2013\n",
      "X $1GMSC.arff OBagAd 50 1538 3179 3625\n",
      "X $1GMSC.arff OBagASHT 50 223 368 728\n",
      "X $1GMSC.arff OBagASHT 50 1115 1843 3642\n",
      "X $1GMSC.arff OBagASHT 50 2008 3318 6556\n",
      "X $1GMSC.arff OBag 50 225 368 699\n",
      "X $1GMSC.arff OBag 50 1127 1843 3495\n",
      "X $1GMSC.arff OBag 50 2028 3318 6291\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/x5n78dm15dz0whvgzf_nj9j80000gp/T/ipykernel_55477/1232116270.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['algorithm'] = df[\"algorithm\"].str.replace(\"OB$\", \"OBSequential\")\n",
      "/var/folders/bq/x5n78dm15dz0whvgzf_nj9j80000gp/T/ipykernel_55477/1232116270.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['algorithm'] = df[\"algorithm\"].str.replace(\"ARF$\", \"ARFSequential\")\n",
      "/var/folders/bq/x5n78dm15dz0whvgzf_nj9j80000gp/T/ipykernel_55477/1232116270.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['algorithm'] = df['algorithm'].str.replace(\"Adwin$\", \"AdwinSequential\")\n",
      "/var/folders/bq/x5n78dm15dz0whvgzf_nj9j80000gp/T/ipykernel_55477/1232116270.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['algorithm'] = df['algorithm'].str.replace(\"SRP$\", \"SRPSequential\")\n",
      "/var/folders/bq/x5n78dm15dz0whvgzf_nj9j80000gp/T/ipykernel_55477/1232116270.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['algorithm'] = df['algorithm'].str.replace(\"OBASHT$\", \"OBASHTSequential\")\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/reginaldoluisdeluna/Documents/Ufscar/comparison-xue3m-minibatching/\n",
    "\n",
    "for i in range(len(moaDumpFolders)):\n",
    "    architecture=moaDumpFolders[i].split('/')[0]\n",
    "    print(f\"------------------------------ {architecture} ------------------------------ \")\n",
    "    parse_folder_to_file(f\"results/{moaDumpFolders[i]}\", f\"parsed_csvs/{wantedCSVfilename[i]}\")\n",
    "    calculate_rate_csv(f'parsed_csvs/{wantedCSVfilename[i]}', architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
