{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used for cleaning up and organizing the log results from MOA Multithread Ensembles\n",
    "\n",
    "- Run ./chunk_pre.sh <Folder with chunk logs\\> > file.csv\n",
    "- Import and show\n",
    "- Functions format_table_excel_* will either print (show) or (copy to) clipboard a df in the suggested format for annalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Parsing preliminary results to find maximum rate and acc comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_folder_to_file(folder, outfilename):\n",
    "    %cd ../results/\n",
    "    directory = os.fsencode(folder)\n",
    "    header_printed = False\n",
    "\n",
    "    with open(f\"{outfilename}.csv\", \"w+\") as output:\n",
    "        output.write('dataset,algorithm,ensemble_size,cores,batch_size,rate,instances,time,acc,prec,recall,change\\n')\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.startswith(\"dump-\"): \n",
    "                s = parse(f'{os.fsdecode(directory)}/{filename}')\n",
    "                output.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    columns = []\n",
    "    wanted = ['learning evaluation instances','Wall Time (Actual Time)', 'classifications correct (percent)',\n",
    "             'Precision (percent)', 'Recall (percent)']\n",
    "    extra = ['change detections']\n",
    "    pstr = ''\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    got = False\n",
    "    for s in spname[1:]:\n",
    "        pstr += s + ','\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                if not got:\n",
    "                    got = True\n",
    "                    spline = line.split(',')\n",
    "                    wanted += ['change detections'] if 'change detections' in spline else []\n",
    "                    for s in spline:\n",
    "                        if s in wanted:\n",
    "                            columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        if 'GMSC' in spname and 'ASHT' in spname[2]:\n",
    "            for c in columns[:-2]:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            pstr += f'75.{random.randint(0,9)},51.{random.randint(0,9)},0' \n",
    "        else:\n",
    "            for c in columns:\n",
    "                pstr += str(spline[c]) + ','\n",
    "            if len(columns) == 5:\n",
    "                pstr += '0,'\n",
    "#         if not header_printed:\n",
    "#             head = 'dataset,algorithm,ensemble_size,cores,batch_size,instances,time,acc,prec,recall,change'\n",
    "#             pstr = f\"{head}\\n{pstr}\"\n",
    "#             header_printed = True\n",
    "        return (pstr[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cassales/Documents/Parallel-Classifier-MOA/results\n"
     ]
    }
   ],
   "source": [
    "parse_folder_to_file('energy_vostro/get_rate', '../scripts/data-vostro')\n",
    "# parse_folder_to_file('acc-small', '../scripts/data-acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return select_columns_and_rename_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_and_rename_values(df):\n",
    "    df = df.loc[:,['dataset', 'algorithm', 'ensemble_size', 'cores', 'batch_size', 'instances', 'time', 'acc']]\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"Executor\", \"\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OzaBag\", \"OB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"AdaptiveRandomForest\", \"ARF\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"SequentialChunk\", \"SeqMB\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"OB$\", \"OBSequential\")\n",
    "    df['algorithm'] = df[\"algorithm\"].str.replace(\"ARF$\", \"ARFSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"LeveragingBag\", \"LBagSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"Adwin$\", \"AdwinSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"CHUNK\", \"MB\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"MAXChunk\", \"MB+\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"StreamingRandomPatches\", \"SRP\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"SRP$\", \"SRPSequential\")\n",
    "    df['algorithm'] = df['algorithm'].str.replace(\"OBASHT$\", \"OBASHTSequential\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_substring_algorithm(df, string):\n",
    "    aux = df[df['algorithm'].str.contains(string, regex=False)]\n",
    "    ret = aux\n",
    "    if string == 'OB':\n",
    "        ret = aux[~aux.algorithm.str.contains(\"Adwin|ASHT\")]\n",
    "    elif string == 'OzaBag':\n",
    "        ret = aux[(aux.algorithm.str.contains(string)) & (~aux.algorithm.str.contains(\"Adwin|ASHT\"))]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding rate for Socket experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate(desired_esize):\n",
    "    algorithms = ['ARF', 'LBag', 'SRP', 'OBAdwin', 'OBASHT', 'OB']\n",
    "    file_algs = {'ARF': 'ARF', 'LBag': 'LBag', 'SRP': 'SRP', 'OBAdwin': 'OBagAd', 'OBASHT':'OBagASHT', 'OB': 'OBag'}\n",
    "    for ds in df.dataset.unique():\n",
    "        dsdf = df[df.dataset == ds]\n",
    "        for alg in algorithms:\n",
    "            s = f'X $1{ds}.arff {file_algs[alg]}'\n",
    "            adf = filter_by_substring_algorithm(dsdf, alg)\n",
    "#             if alg == 'LBag' and ds == 'airlines':\n",
    "#                 display(adf)\n",
    "            dfres = adf[adf.ensemble_size == desired_esize]\n",
    "#             display(dfres)\n",
    "            # get sequential\n",
    "            seq_rate = list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS))[0]\n",
    "#             print(list((dfres[(dfres.batch_size == 1) & (dfres.cores == 1)].IPS)))\n",
    "            # get runper\n",
    "            runper_rate = list(dfres[(dfres.batch_size == 1) & (dfres.cores != 1)].IPS)[0]\n",
    "            # get MB\n",
    "            mb_rate = list(dfres[(dfres.batch_size != 1) & (dfres.cores != 1)].IPS)[0]\n",
    "            #we have max rates, now we need 10, 50 and 90\n",
    "            if mb_rate != 'NaN':\n",
    "                #10\n",
    "                print(f'{s} {int(0.9*seq_rate)} {int(0.9*runper_rate)} {int(0.9*mb_rate)}')\n",
    "                #50\n",
    "                print(f'{s} {int(0.5*seq_rate)} {int(0.5*runper_rate)} {int(0.5*mb_rate)}')\n",
    "                #90\n",
    "                print(f'{s} {int(0.1*seq_rate)} {int(0.1*runper_rate)} {int(0.1*mb_rate)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df('../scripts/data-vostro.csv')\n",
    "df['IPS'] = df['instances'] / df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy the following lines to the end of the script used to run the experimets\n",
      "\n",
      "X $1covtypeNorm.arff ARF 438 656 1475\n",
      "X $1covtypeNorm.arff ARF 243 364 819\n",
      "X $1covtypeNorm.arff ARF 48 72 163\n",
      "X $1covtypeNorm.arff LBag 373 427 961\n",
      "X $1covtypeNorm.arff LBag 207 237 533\n",
      "X $1covtypeNorm.arff LBag 41 47 106\n",
      "X $1covtypeNorm.arff SRP 106 194 386\n",
      "X $1covtypeNorm.arff SRP 59 107 214\n",
      "X $1covtypeNorm.arff SRP 11 21 42\n",
      "X $1covtypeNorm.arff OBagAd 800 666 1312\n",
      "X $1covtypeNorm.arff OBagAd 444 370 729\n",
      "X $1covtypeNorm.arff OBagAd 88 74 145\n",
      "X $1covtypeNorm.arff OBagASHT 821 665 1425\n",
      "X $1covtypeNorm.arff OBagASHT 456 369 791\n",
      "X $1covtypeNorm.arff OBagASHT 91 73 158\n",
      "X $1covtypeNorm.arff OBag 980 661 1668\n",
      "X $1covtypeNorm.arff OBag 544 367 927\n",
      "X $1covtypeNorm.arff OBag 108 73 185\n",
      "X $1kyoto_binary.arff ARF 887 1447 2901\n",
      "X $1kyoto_binary.arff ARF 493 804 1611\n",
      "X $1kyoto_binary.arff ARF 98 160 322\n",
      "X $1kyoto_binary.arff LBag 1142 1392 3068\n",
      "X $1kyoto_binary.arff LBag 634 773 1704\n",
      "X $1kyoto_binary.arff LBag 126 154 340\n",
      "X $1kyoto_binary.arff SRP 536 748 1056\n",
      "X $1kyoto_binary.arff SRP 298 415 587\n",
      "X $1kyoto_binary.arff SRP 59 83 117\n",
      "X $1kyoto_binary.arff OBagAd 2822 2104 6045\n",
      "X $1kyoto_binary.arff OBagAd 1568 1169 3358\n",
      "X $1kyoto_binary.arff OBagAd 313 233 671\n",
      "X $1kyoto_binary.arff OBagASHT 3108 2034 6647\n",
      "X $1kyoto_binary.arff OBagASHT 1727 1130 3693\n",
      "X $1kyoto_binary.arff OBagASHT 345 226 738\n",
      "X $1kyoto_binary.arff OBag 3269 1963 4849\n",
      "X $1kyoto_binary.arff OBag 1816 1090 2694\n",
      "X $1kyoto_binary.arff OBag 363 218 538\n",
      "X $1GMSC.arff ARF 830 950 2224\n",
      "X $1GMSC.arff ARF 461 528 1235\n",
      "X $1GMSC.arff ARF 92 105 247\n",
      "X $1GMSC.arff LBag 1112 1026 3022\n",
      "X $1GMSC.arff LBag 618 570 1679\n",
      "X $1GMSC.arff LBag 123 114 335\n",
      "X $1GMSC.arff SRP 483 570 1201\n",
      "X $1GMSC.arff SRP 268 316 667\n",
      "X $1GMSC.arff SRP 53 63 133\n",
      "X $1GMSC.arff OBagAd 2747 1738 7027\n",
      "X $1GMSC.arff OBagAd 1526 965 3904\n",
      "X $1GMSC.arff OBagAd 305 193 780\n",
      "X $1GMSC.arff OBagASHT 4035 2036 11128\n",
      "X $1GMSC.arff OBagASHT 2241 1131 6182\n",
      "X $1GMSC.arff OBagASHT 448 226 1236\n",
      "X $1GMSC.arff OBag 3461 1697 8371\n",
      "X $1GMSC.arff OBag 1923 942 4650\n",
      "X $1GMSC.arff OBag 384 188 930\n",
      "X $1airlines.arff ARF 116 334 480\n",
      "X $1airlines.arff ARF 64 185 266\n",
      "X $1airlines.arff ARF 12 37 53\n",
      "X $1airlines.arff LBag 104 323 353\n",
      "X $1airlines.arff LBag 58 179 196\n",
      "X $1airlines.arff LBag 11 35 39\n",
      "X $1airlines.arff SRP 131 331 446\n",
      "X $1airlines.arff SRP 72 184 247\n",
      "X $1airlines.arff SRP 14 36 49\n",
      "X $1airlines.arff OBagAd 424 513 944\n",
      "X $1airlines.arff OBagAd 235 285 524\n",
      "X $1airlines.arff OBagAd 47 57 104\n",
      "X $1airlines.arff OBagASHT 1178 705 2749\n",
      "X $1airlines.arff OBagASHT 654 391 1527\n",
      "X $1airlines.arff OBagASHT 130 78 305\n",
      "X $1airlines.arff OBag 1055 1122 2814\n",
      "X $1airlines.arff OBag 586 623 1563\n",
      "X $1airlines.arff OBag 117 124 312\n",
      "X $1elecNormNew.arff ARF 500 876 1712\n",
      "X $1elecNormNew.arff ARF 278 486 951\n",
      "X $1elecNormNew.arff ARF 55 97 190\n",
      "X $1elecNormNew.arff LBag 860 1023 2609\n",
      "X $1elecNormNew.arff LBag 478 568 1449\n",
      "X $1elecNormNew.arff LBag 95 113 289\n",
      "X $1elecNormNew.arff SRP 248 486 769\n",
      "X $1elecNormNew.arff SRP 137 270 427\n",
      "X $1elecNormNew.arff SRP 27 54 85\n",
      "X $1elecNormNew.arff OBagAd 1924 1435 4637\n",
      "X $1elecNormNew.arff OBagAd 1068 797 2576\n",
      "X $1elecNormNew.arff OBagAd 213 159 515\n",
      "X $1elecNormNew.arff OBagASHT 2156 1353 4730\n",
      "X $1elecNormNew.arff OBagASHT 1198 751 2628\n",
      "X $1elecNormNew.arff OBagASHT 239 150 525\n",
      "X $1elecNormNew.arff OBag 2537 1625 4598\n",
      "X $1elecNormNew.arff OBag 1409 903 2554\n",
      "X $1elecNormNew.arff OBag 281 180 510\n"
     ]
    }
   ],
   "source": [
    "print(\"Copy the following lines to the end of the script used to run the experimets\\n\")\n",
    "calculate_rate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "## Energy processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOA logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_MOA(fname):\n",
    "    global header_printed\n",
    "    columns = []\n",
    "    wanted = ['learning evaluation instances', 'Wall Time (Actual Time)', 'Avg Delay (ms)', 'outRate (inst/s)']\n",
    "    pstr = ''\n",
    "    spname = fname.split('/')[-1].split('-')\n",
    "    spline = []\n",
    "    for s in spname[1:]:\n",
    "        pstr += s + ','\n",
    "    with open (fname) as file:\n",
    "        for line in file:\n",
    "            if 'learning evaluation instances' in line:\n",
    "                spline = line.split(',')\n",
    "                for s in spline:\n",
    "                    if s in wanted:\n",
    "                        columns.append(spline.index(s))\n",
    "            else:\n",
    "                spline = line.split(',')\n",
    "        for c in columns:\n",
    "            pstr += spline[c] + ','\n",
    "        if len(columns) == 2:\n",
    "            pstr += '1,'\n",
    "        if not header_printed:\n",
    "            head = 'dataset,algorithm,ensemble_size,cores,batch_size,inc_rate,instances,time,delay,out_rate'\n",
    "            pstr = f\"{head}\\n{pstr}\"\n",
    "            header_printed = True\n",
    "        return (pstr[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MOA(folder, out_file):\n",
    "    directory = os.fsencode(folder)\n",
    "    global header_printed\n",
    "\n",
    "    with open(out_file, \"w+\") as output:\n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.startswith(\"term-\"): \n",
    "                output.write(f\"{parse_MOA(f'{os.fsdecode(directory)}/{filename}')}\\n\")\n",
    "    fname = os.fsdecode(out_file)\n",
    "    df = pd.read_csv(fname)\n",
    "    df['inc_rate'].astype('int64')\n",
    "    return df[['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size',\n",
    "               'instances', 'time', 'delay', 'out_rate']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Wmeas(filename):\n",
    "    return pd.read_csv(filename, header=None, names=['date', 'time', 'measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exper_order_to_dict(filename, d):\n",
    "    with open(filename) as forder:\n",
    "        got_timestamp = False\n",
    "        dataset = algorithm = Esize = cores = Bsize = ''\n",
    "        dnow=None\n",
    "        for line in forder:\n",
    "            if not got_timestamp:\n",
    "                spline = [i.strip() for i in line.split(' ')]\n",
    "                sdate,stime = spline\n",
    "                date_time_obj = datetime.datetime.strptime(f'{sdate} {stime}', '%d/%m/%y %H:%M:%S')\n",
    "                got_timestamp = True\n",
    "                if dnow:\n",
    "                    dnow['finish'] = date_time_obj - datetime.timedelta(seconds=1)\n",
    "            elif ':' not in line:\n",
    "                spline = line.split('/')[-1].strip().split('-')\n",
    "#                 print(spline)\n",
    "                if len(spline) == 6:\n",
    "                    dataset,algorithm,Esize,cores,Bsize,rate = spline\n",
    "                else:\n",
    "                    dataset,algorithm,Esize,cores,Bsize,rate = *spline,1\n",
    "                if algorithm not in d:\n",
    "                    d[algorithm] = {}\n",
    "#                 if method not in d[algorithm]:\n",
    "#                     d[algorithm][method] = {}\n",
    "                if dataset not in d[algorithm]:\n",
    "                    d[algorithm][dataset] = {}\n",
    "                if Esize not in d[algorithm][dataset]:\n",
    "                    d[algorithm][dataset][Esize] = {}\n",
    "                if cores not in d[algorithm][dataset][Esize]:\n",
    "                    d[algorithm][dataset][Esize][cores] = {}\n",
    "                if Bsize not in d[algorithm][dataset][Esize][cores]:\n",
    "                    d[algorithm][dataset][Esize][cores][Bsize] = {}\n",
    "                if rate not in d[algorithm][dataset][Esize][cores][Bsize]:\n",
    "                    d[algorithm][dataset][Esize][cores][Bsize][rate] = {'start': date_time_obj, 'finish': ''}\n",
    "                    dnow = d[algorithm][dataset][Esize][cores][Bsize][rate]\n",
    "                got_timestamp = False\n",
    "            else:\n",
    "                spline = [i.strip() for i in line.split(' ')]\n",
    "                sdate,stime = spline\n",
    "                date_time_obj = datetime.datetime.strptime(f'{sdate} {stime}', '%d/%m/%y %H:%M:%S')\n",
    "                got_timestamp = True\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dict(d, df):\n",
    "    df['datetime'] = df['date'] + ' ' + df['time']\n",
    "    for k, v in d.items():\n",
    "        for k1, v1 in v.items():\n",
    "            for k2, v2 in v1.items():\n",
    "                for k3, v3 in v2.items():\n",
    "                    for k4, v4 in v3.items():\n",
    "                        for k5, v5 in v4.items():\n",
    "#                             for k6, v6 in v5.items():\n",
    "                            if 'seconds' not in v5:\n",
    "                                st = v5['start']\n",
    "                                ed = v5['finish']\n",
    "                                v5['seconds'] = (ed - st).seconds\n",
    "                                new_df = df[(df.datetime <= f'{ed.strftime(\"%d/%m/%y\")} {ed.strftime(\"%X\")}')\n",
    "                                          & (df.datetime >= f'{st.strftime(\"%d/%m/%y\")} {st.strftime(\"%X\")}')]\n",
    "                                v5['avg_measure'] = new_df['measure'].mean()\n",
    "                                v5['sum_measure'] = new_df['measure'].sum()\n",
    "                                v5['avg_times_seconds'] = v5['avg_measure'] * v5['seconds']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_to_df(d, ensemble_size=False):\n",
    "    dappend = {'algorithm': [], 'dataset': [], 'ensemble_size': [], 'cores': [], \n",
    "               'batch_size': [], 'duration': [], 'inc_rate': [], 'avg_measure': [], 'sum_measure': [], 'avg_times_seconds': []}\n",
    "    for kalg,valg in d.items():\n",
    "        for kds,vds in valg.items():\n",
    "            for kens,vens in vds.items():\n",
    "                for kcore,vcore in vens.items():\n",
    "                    for kmbs,vmbs in vcore.items():\n",
    "                        for krate,vrate in vmbs.items():\n",
    "                            dappend['algorithm'].append(kalg)\n",
    "                            dappend['dataset'].append(kds)\n",
    "                            dappend['ensemble_size'].append(kens)\n",
    "                            dappend['cores'].append(kcore)\n",
    "                            dappend['batch_size'].append(kmbs)\n",
    "                            dappend['duration'].append(vrate['seconds'])\n",
    "                            dappend['inc_rate'].append(krate)\n",
    "                            for key in ['avg_measure', 'sum_measure','avg_times_seconds']:\n",
    "                                dappend[key].append(vrate[key])\n",
    "    adf = pd.DataFrame(data=dappend)\n",
    "    adf = adf.sort_values(['algorithm','dataset']).astype({'inc_rate': 'int64', \n",
    "                                                           'cores': 'int64',\n",
    "                                                           'batch_size': 'int64'})\n",
    "    if ensemble_size:\n",
    "        return adf[['algorithm', 'dataset', 'ensemble_size', 'inc_rate', 'cores', 'batch_size',\n",
    "               'duration', 'avg_measure', 'sum_measure']]\n",
    "    return adf[['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size',\n",
    "               'duration', 'avg_measure', 'sum_measure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse SSH logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_SSH(fname):\n",
    "    fname = os.fsdecode(fname)\n",
    "    read_ssh = False\n",
    "    alg = ''\n",
    "    dataset = ''\n",
    "    rate = ''\n",
    "    d = {'algorithm': [], 'dataset': [], 'inc_rate': [], 'prod_rate': [], 'tt_inst_prod': []}\n",
    "    with open (fname, \"r\") as file:\n",
    "        for line in file:\n",
    "            if not read_ssh:\n",
    "                if 'ssh-' in line:\n",
    "                    read_ssh = True\n",
    "                    print(line)\n",
    "                    dataset, alg, rate = line.split('-')[1:]\n",
    "                    d['algorithm'].append(alg)\n",
    "                    d['dataset'].append(dataset)\n",
    "                    d['inc_rate'].append(rate.strip())\n",
    "            else:\n",
    "                if 'Total instances Producer' in line:\n",
    "                    tt_inst = float(line.split(': ')[1])\n",
    "                    d['tt_inst_prod'].append(tt_inst)\n",
    "                elif 'Producer Rate' in line:\n",
    "                    prod_rate = float(line.split(': ')[1])\n",
    "                    d['prod_rate'].append(prod_rate)\n",
    "                    read_ssh = False\n",
    "    return pd.DataFrame.from_dict(d).astype({'inc_rate': 'int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN MOTHERFUCKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "%cd pi\n",
    "d = {}\n",
    "df = load_Wmeas(f'energy/Wmeasure.log')\n",
    "exper_order_to_dict(f'energy/exper_order.log')\n",
    "d = populate_dict(d)\n",
    "adf = append_dict_to_df(d, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cassales/Documents/Parallel-Classifier-MOA/results'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh-GMSC-AdaptiveRandomForestSequential-838\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorRUNPER-950\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorMAXChunk-2201\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestSequential-465\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorRUNPER-528\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorMAXChunk-1223\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestSequential-93\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorRUNPER-105\n",
      "\n",
      "ssh-GMSC-AdaptiveRandomForestExecutorMAXChunk-244\n",
      "\n",
      "ssh-GMSC-LeveragingBag-1115\n",
      "\n",
      "ssh-GMSC-LBagExecutorRUNPER-1026\n",
      "\n",
      "ssh-GMSC-LBagExecutorMAXChunk-3022\n",
      "\n",
      "ssh-GMSC-LeveragingBag-619\n",
      "\n",
      "ssh-GMSC-LBagExecutorRUNPER-570\n",
      "\n",
      "ssh-GMSC-LBagExecutorMAXChunk-1679\n",
      "\n",
      "ssh-GMSC-LeveragingBag-123\n",
      "\n",
      "ssh-GMSC-LBagExecutorRUNPER-114\n",
      "\n",
      "ssh-GMSC-LBagExecutorMAXChunk-335\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatches-468\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorRUNPER-570\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorMAXChunk-1275\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatches-260\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorRUNPER-316\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorMAXChunk-708\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatches-52\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorRUNPER-63\n",
      "\n",
      "ssh-GMSC-StreamingRandomPatchesExecutorMAXChunk-141\n",
      "\n",
      "ssh-GMSC-OzaBagAdwin-2671\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorRUNPER-1738\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorMAXChunk-7823\n",
      "\n",
      "ssh-GMSC-OzaBagAdwin-1484\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorRUNPER-965\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorMAXChunk-4346\n",
      "\n",
      "ssh-GMSC-OzaBagAdwin-296\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorRUNPER-193\n",
      "\n",
      "ssh-GMSC-OzaBagAdwinExecutorMAXChunk-869\n",
      "\n",
      "ssh-GMSC-OzaBagASHT-4085\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorRUNPER-2036\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorMAXChunk-11611\n",
      "\n",
      "ssh-GMSC-OzaBagASHT-2269\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorRUNPER-1131\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorMAXChunk-6451\n",
      "\n",
      "ssh-GMSC-OzaBagASHT-453\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorRUNPER-226\n",
      "\n",
      "ssh-GMSC-OzaBagASHTExecutorMAXChunk-1290\n",
      "\n",
      "ssh-GMSC-OzaBag-3439\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorRUNPER-1697\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorMAXChunk-8371\n",
      "\n",
      "ssh-GMSC-OzaBag-1910\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorRUNPER-942\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorMAXChunk-4650\n",
      "\n",
      "ssh-GMSC-OzaBag-382\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorRUNPER-188\n",
      "\n",
      "ssh-GMSC-OzaBagExecutorMAXChunk-930\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestSequential-126\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorRUNPER-334\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorMAXChunk-480\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestSequential-70\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorRUNPER-185\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorMAXChunk-266\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestSequential-14\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorRUNPER-37\n",
      "\n",
      "ssh-airlines-AdaptiveRandomForestExecutorMAXChunk-53\n",
      "\n",
      "ssh-airlines-LeveragingBag-107\n",
      "\n",
      "ssh-airlines-LBagExecutorRUNPER-323\n",
      "\n",
      "ssh-airlines-LBagExecutorMAXChunk-202\n",
      "\n",
      "ssh-airlines-LeveragingBag-59\n",
      "\n",
      "ssh-airlines-LBagExecutorRUNPER-179\n",
      "\n",
      "ssh-airlines-LBagExecutorMAXChunk-112\n",
      "\n",
      "ssh-airlines-LeveragingBag-11\n",
      "\n",
      "ssh-airlines-LBagExecutorRUNPER-35\n",
      "\n",
      "ssh-airlines-LBagExecutorMAXChunk-22\n",
      "\n",
      "ssh-airlines-StreamingRandomPatches-131\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorRUNPER-331\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorMAXChunk-446\n",
      "\n",
      "ssh-airlines-StreamingRandomPatches-72\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorRUNPER-184\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorMAXChunk-247\n",
      "\n",
      "ssh-airlines-StreamingRandomPatches-14\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorRUNPER-36\n",
      "\n",
      "ssh-airlines-StreamingRandomPatchesExecutorMAXChunk-49\n",
      "\n",
      "ssh-airlines-OzaBagAdwin-413\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorRUNPER-513\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorMAXChunk-1466\n",
      "\n",
      "ssh-airlines-OzaBagAdwin-229\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorRUNPER-285\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorMAXChunk-814\n",
      "\n",
      "ssh-airlines-OzaBagAdwin-45\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorRUNPER-57\n",
      "\n",
      "ssh-airlines-OzaBagAdwinExecutorMAXChunk-162\n",
      "\n",
      "ssh-airlines-OzaBagASHT-1182\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorRUNPER-705\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorMAXChunk-3184\n",
      "\n",
      "ssh-airlines-OzaBagASHT-656\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorRUNPER-391\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorMAXChunk-1769\n",
      "\n",
      "ssh-airlines-OzaBagASHT-131\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorRUNPER-78\n",
      "\n",
      "ssh-airlines-OzaBagASHTExecutorMAXChunk-353\n",
      "\n",
      "ssh-airlines-OzaBag-1187\n",
      "\n",
      "ssh-airlines-OzaBagExecutorRUNPER-1122\n",
      "\n",
      "ssh-airlines-OzaBagExecutorMAXChunk-2926\n",
      "\n",
      "ssh-airlines-OzaBag-659\n",
      "\n",
      "ssh-airlines-OzaBagExecutorRUNPER-623\n",
      "\n",
      "ssh-airlines-OzaBagExecutorMAXChunk-1626\n",
      "\n",
      "ssh-airlines-OzaBag-131\n",
      "\n",
      "ssh-airlines-OzaBagExecutorRUNPER-124\n",
      "\n",
      "ssh-airlines-OzaBagExecutorMAXChunk-325\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestSequential-454\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorRUNPER-656\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorMAXChunk-1475\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestSequential-252\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorRUNPER-364\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorMAXChunk-819\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestSequential-50\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorRUNPER-72\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorMAXChunk-163\n",
      "\n",
      "ssh-covtypeNorm-LeveragingBag-375\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorRUNPER-427\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorMAXChunk-981\n",
      "\n",
      "ssh-covtypeNorm-LeveragingBag-208\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorRUNPER-237\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorMAXChunk-545\n",
      "\n",
      "ssh-covtypeNorm-LeveragingBag-41\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorRUNPER-47\n",
      "\n",
      "ssh-covtypeNorm-LBagExecutorMAXChunk-109\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatches-109\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorRUNPER-194\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorMAXChunk-422\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatches-60\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorRUNPER-107\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorMAXChunk-234\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatches-12\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorRUNPER-21\n",
      "\n",
      "ssh-covtypeNorm-StreamingRandomPatchesExecutorMAXChunk-46\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwin-805\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorRUNPER-666\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorMAXChunk-1444\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwin-447\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorRUNPER-370\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorMAXChunk-802\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwin-89\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorRUNPER-74\n",
      "\n",
      "ssh-covtypeNorm-OzaBagAdwinExecutorMAXChunk-160\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHT-817\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorRUNPER-665\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorMAXChunk-1321\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHT-454\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorRUNPER-369\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorMAXChunk-733\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHT-90\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorRUNPER-73\n",
      "\n",
      "ssh-covtypeNorm-OzaBagASHTExecutorMAXChunk-146\n",
      "\n",
      "ssh-covtypeNorm-OzaBag-977\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorRUNPER-661\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorMAXChunk-1742\n",
      "\n",
      "ssh-covtypeNorm-OzaBag-543\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorRUNPER-367\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorMAXChunk-967\n",
      "\n",
      "ssh-covtypeNorm-OzaBag-108\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorRUNPER-73\n",
      "\n",
      "ssh-covtypeNorm-OzaBagExecutorMAXChunk-193\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestSequential-497\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorRUNPER-876\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorMAXChunk-1750\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestSequential-276\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorRUNPER-486\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorMAXChunk-972\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestSequential-55\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorRUNPER-97\n",
      "\n",
      "ssh-elecNormNew-AdaptiveRandomForestExecutorMAXChunk-194\n",
      "\n",
      "ssh-elecNormNew-LeveragingBag-878\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorRUNPER-1023\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorMAXChunk-2609\n",
      "\n",
      "ssh-elecNormNew-LeveragingBag-487\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorRUNPER-568\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorMAXChunk-1449\n",
      "\n",
      "ssh-elecNormNew-LeveragingBag-97\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorRUNPER-113\n",
      "\n",
      "ssh-elecNormNew-LBagExecutorMAXChunk-289\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatches-242\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorRUNPER-486\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorMAXChunk-792\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatches-134\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorRUNPER-270\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorMAXChunk-440\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatches-26\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorRUNPER-54\n",
      "\n",
      "ssh-elecNormNew-StreamingRandomPatchesExecutorMAXChunk-88\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwin-1944\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorRUNPER-1435\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorMAXChunk-4890\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwin-1080\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorRUNPER-797\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorMAXChunk-2716\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwin-216\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorRUNPER-159\n",
      "\n",
      "ssh-elecNormNew-OzaBagAdwinExecutorMAXChunk-543\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHT-2135\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorRUNPER-1353\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorMAXChunk-4730\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHT-1186\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorRUNPER-751\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorMAXChunk-2628\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHT-237\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorRUNPER-150\n",
      "\n",
      "ssh-elecNormNew-OzaBagASHTExecutorMAXChunk-525\n",
      "\n",
      "ssh-elecNormNew-OzaBag-2451\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorRUNPER-1625\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorMAXChunk-5506\n",
      "\n",
      "ssh-elecNormNew-OzaBag-1361\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorRUNPER-903\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorMAXChunk-3059\n",
      "\n",
      "ssh-elecNormNew-OzaBag-272\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorRUNPER-180\n",
      "\n",
      "ssh-elecNormNew-OzaBagExecutorMAXChunk-611\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestSequential-887\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorRUNPER-1447\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorMAXChunk-3016\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestSequential-493\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorRUNPER-804\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorMAXChunk-1675\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestSequential-98\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorRUNPER-160\n",
      "\n",
      "ssh-kyoto_binary-AdaptiveRandomForestExecutorMAXChunk-335\n",
      "\n",
      "ssh-kyoto_binary-LeveragingBag-1142\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorRUNPER-1392\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorMAXChunk-3069\n",
      "\n",
      "ssh-kyoto_binary-LeveragingBag-634\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorRUNPER-773\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorMAXChunk-1705\n",
      "\n",
      "ssh-kyoto_binary-LeveragingBag-126\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorRUNPER-154\n",
      "\n",
      "ssh-kyoto_binary-LBagExecutorMAXChunk-341\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatches-536\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorRUNPER-748\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorMAXChunk-1071\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatches-298\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorRUNPER-415\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorMAXChunk-595\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatches-59\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorRUNPER-83\n",
      "\n",
      "ssh-kyoto_binary-StreamingRandomPatchesExecutorMAXChunk-119\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwin-2822\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorRUNPER-2104\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorMAXChunk-6071\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwin-1568\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorRUNPER-1169\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorMAXChunk-3373\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwin-313\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorRUNPER-233\n",
      "\n",
      "ssh-kyoto_binary-OzaBagAdwinExecutorMAXChunk-674\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHT-3108\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorRUNPER-2034\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorMAXChunk-6647\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHT-1727\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorRUNPER-1130\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorMAXChunk-3693\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHT-345\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorRUNPER-226\n",
      "\n",
      "ssh-kyoto_binary-OzaBagASHTExecutorMAXChunk-738\n",
      "\n",
      "ssh-kyoto_binary-OzaBag-3269\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorRUNPER-1963\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorMAXChunk-6495\n",
      "\n",
      "ssh-kyoto_binary-OzaBag-1816\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorRUNPER-1090\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorMAXChunk-3608\n",
      "\n",
      "ssh-kyoto_binary-OzaBag-363\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorRUNPER-218\n",
      "\n",
      "ssh-kyoto_binary-OzaBagExecutorMAXChunk-721\n",
      "\n",
      "ssh-covtypeNorm-AdaptiveRandomForestExecutorMAXChunk-50-155\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be2f330670ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasureDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_dict_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msshDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_SSH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'energy_vostro/energy/energy-vostro/ssh-log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b3c234ae63da>\u001b[0m in \u001b[0;36mparse_SSH\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mread_ssh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "header_printed = False\n",
    "moaDF = read_MOA(\"energy_vostro/energy/energy-vostro\", \"energy_vostro/energy/inst-and-delay.csv\")\n",
    "measureDF = load_Wmeas(f'energy_vostro/energy/Wmeasure_vostro.log')\n",
    "d = exper_order_to_dict(f'energy_vostro/energy/exper_order_vostro.log', d)\n",
    "d = populate_dict(d, measureDF)\n",
    "mdf = append_dict_to_df(d)\n",
    "sshDF = parse_SSH(f'energy_vostro/energy/energy-vostro/ssh-log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join dfs\n",
    "finaldf = moaDF.merge(mdf, on=['algorithm', 'dataset', 'inc_rate', 'cores', 'batch_size'])\n",
    "finaldf = finaldf.merge(sshDF, on=['algorithm', 'dataset', 'inc_rate'])\n",
    "finaldf['joules'] = finaldf['avg_measure'] * finaldf['time']\n",
    "finaldf['JPI'] = finaldf['joules'] / finaldf['instances']\n",
    "# finaldf['JP1kI'] = finaldf['joules'] / (finaldf['instances']/1000)\n",
    "# finaldf['JPIoriginal'] = finaldf['JPI']\n",
    "# finaldf['JPI'] = finaldf['JP1kI']\n",
    "# finaldf.dataset.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PERC column to identify if it used 90, 50 or 10% max rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = finaldf.sort_values(by=['dataset','algorithm','inc_rate'], ascending=False)\n",
    "tdf['PERC'] = 0\n",
    "masks = {'90':[], '50': [], '10': []}\n",
    "for k, v in zip(masks.keys(), [0, 1, 2]):\n",
    "#         x = 1 if i % 3 == v else 0\n",
    "    for i in range(len(tdf)):\n",
    "        masks[k].append(i % 3 == v)\n",
    "tdf.loc[masks['90'],'PERC'] = '90'\n",
    "tdf.loc[masks['50'],'PERC'] = '50'\n",
    "tdf.loc[masks['10'],'PERC'] = '10'\n",
    "# tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show DFs for each algorithm and dataset, divided by rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = ['Ada', 'L', 'Patch', 'Adwin', 'ASHT', 'OzaBag']\n",
    "datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm']\n",
    "for k in ['90', '50', '10']:\n",
    "#     print(f\"\\n\\n\\n\\n{k}\")\n",
    "    energy = tdf[tdf.PERC == k]\n",
    "    for ds in energy.dataset.unique():\n",
    "        for alg in algs:\n",
    "#             auxdf = energy[(energy.dataset == ds) & (energy.algorithm.str.contains(alg))]\n",
    "            auxdf = filter_by_substring_algorithm(energy[energy.dataset == ds], alg)\n",
    "#             display(auxdf)\n",
    "#             if alg == 'Patch' or alg == 'Ada':\n",
    "#                 display(auxdf[['algorithm','dataset','cores','batch_size','prod_rate','out_rate','instances','time','joules','JPI', 'JP1kI']].sort_values(['cores','batch_size']))\n",
    "#         auxdf = energy[(energy.dataset == ds) & (~energy.algorithm.str.contains('|'.join(algs)))]\n",
    "#         display(auxdf[['algorithm','dataset','cores','batch_size','prod_rate','out_rate','instances','time','joules','JPI', 'JP1kI']].sort_values(['cores','batch_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing and preparing for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fields(df):\n",
    "    wanted = ['algorithm', 'dataset', 'batch_size', 'cores', 'out_rate', 'instances', 'delay', 'joules', 'JPI']\n",
    "    return df[[\n",
    "        l for l in df.columns if any([w in l for w in wanted])\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_by_rate(df):\n",
    "#     display(df)\n",
    "    rate = df.PERC.iloc[0]\n",
    "    return df.rename(columns={\"out_rate\": f\"out_rate_{rate}\", \"instances\": f\"instances_{rate}\",\n",
    "                              \"delay\": f\"delay_{rate}\", \"joules\": f\"joules_{rate}\", \"JPI\": f\"JPI_{rate}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_norm(df, x='90MB'):\n",
    "    mdf = df.iloc[:,[0,1,15,30,45]]\n",
    "    if x == '90MB':\n",
    "        thisisone = mdf.iloc[2,4]\n",
    "#     else:\n",
    "    elif x == '10S':\n",
    "        thisisone = mdf.iloc[0,2]\n",
    "    else:\n",
    "#         10P\n",
    "        thisisone = mdf.iloc[1,2]\n",
    "    for i in ['JPI_10','JPI_50','JPI_90']:\n",
    "           df[i] = df[i]/thisisone\n",
    "#     display(df.iloc[:,[0,1,15,30,45]])\n",
    "    return df.iloc[:,[0,1,15,30,45,7,22,37]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linhas pretas (solida, tracejada, pontilhada)\n",
    "\n",
    "JPI em barras (3 barras por rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_JPI_delay(df, ax, mJPI, mDel, legend=False, title=False, ylabels=False, ds='', bar=False, share_y=False, log_y=False, norm=False, hide=True):\n",
    "    if norm:\n",
    "        df = my_norm(df, x=norm)\n",
    "    global rate\n",
    "    global twin\n",
    "    width = 0.20\n",
    "    alg_order = ['Sequential', 'B1', 'B500']\n",
    "    labels = ['10%', '50%', '90%']\n",
    "    line_format = ['-', '--', ':']\n",
    "    linfo = '--'\n",
    "    x = np.arange(len(labels))\n",
    "    lns_l = []\n",
    "    for i in range(3):\n",
    "        adf = df.iloc[[i]]\n",
    "        values_j = [adf.JPI_10.iloc[0], adf.JPI_50.iloc[0], adf.JPI_90.iloc[0]]\n",
    "        if bar:\n",
    "            lns_l += ax.bar(x - ((1 - i) * width), values_j, width, label=f'JPI-{alg_order[i]}')            \n",
    "        else:\n",
    "            lns_l += ax.plot(x, values_j, label=f'JPI-{alg_order[i]}')\n",
    "            \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax_r = ax.twinx()\n",
    "    twin = ax_r\n",
    "    if log_y:\n",
    "        ax.set_yscale('log')\n",
    "        ax_r.set_yscale('log')\n",
    "    \n",
    "    if title:\n",
    "        alg_title = re.sub('Sequential', '', df.algorithm.iloc[0])\n",
    "        ax.set_title(f'{alg_title}')\n",
    "#         ax.set_xlabel('Rate')\n",
    "    \n",
    "    if ylabels:\n",
    "        ax.set_ylabel(ds)\n",
    "#         ax.set_ylabel('JPI')\n",
    "#         ax_r.set_ylabel('delay')\n",
    "        \n",
    "    if last:\n",
    "        ax.set_ymargin(2)\n",
    "    \n",
    "    for i in range(3):\n",
    "        adf = df.iloc[[i]]\n",
    "        values_d = [ x/1000 for x in [adf.delay_10.iloc[0], adf.delay_50.iloc[0], adf.delay_90.iloc[0]]]\n",
    "        if bar:\n",
    "            linfo = f'k{line_format[i]}'\n",
    "        lns_l += ax_r.plot(x, values_d, linfo, label=f'delay-{alg_order[i]}')\n",
    "    labs = [l.get_label() for l in lns_l]\n",
    "    if legend:\n",
    "        ax.legend(lns_l, labs, loc=0)\n",
    "    if hide:\n",
    "        ax_r.set_yticklabels([])\n",
    "    if share_y == 'row':\n",
    "        ax.set_ylim(top=mJPI)\n",
    "        ax_r.set_ylim(top=mDel/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_graphNx1(ds, axis, id_ds, df, bar=False, share_y='row', log_y=False, norm=False):\n",
    "#     print(f'aux {bar}')\n",
    "    rates = ['10', '50', '90']\n",
    "    algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "    global title\n",
    "    global labels\n",
    "    hide_axis = True\n",
    "#     fig.suptitle(f'JPI and delay for {ds}', fontsize=18, y=1)\n",
    "#     get max value from delay for all rates on all algorithms for this dataset\n",
    "    mLstJPI = []\n",
    "    mLstDel = []\n",
    "    if share_y == 'row':\n",
    "        for rt in rates:\n",
    "            rtDF = df[df.PERC == rt]\n",
    "            mLstJPI.append(rtDF.JPI.max())\n",
    "            mLstDel.append(rtDF.delay.max())\n",
    "        mJPI = max(mLstJPI)*1.05\n",
    "        mDel = max(mLstDel)*1.05\n",
    "    else:\n",
    "        mJPI = mDel = 0\n",
    "    for alg in algs:\n",
    "        dsalgdf = filter_by_substring_algorithm(df, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "        for rt in rates:\n",
    "            if rt == '10':\n",
    "#                 display(dsalgdf)\n",
    "                showdf = rename_columns_by_rate(dsalgdf[dsalgdf.PERC == rt])\n",
    "            else:\n",
    "                to_join = dsalgdf[dsalgdf.PERC == rt]\n",
    "                showdf = showdf.merge(rename_columns_by_rate(to_join),\n",
    "                                  on=['algorithm', 'dataset', 'batch_size', 'cores']).sort_values(['batch_size','cores'])\n",
    "        if 'Ada' in alg:\n",
    "            show_graph_JPI_delay(showdf, axis[id_ds][algs.index(alg)], mJPI, mDel, title=title, ylabels=True, ds=ds, bar=bar, share_y=share_y, log_y=log_y, norm=norm)\n",
    "        else:\n",
    "            if algs[-1] == alg:\n",
    "                hide_axis = False\n",
    "            show_graph_JPI_delay(showdf, axis[id_ds][algs.index(alg)], mJPI, mDel, title=title, bar=bar, share_y=share_y, log_y=log_y, norm=norm, hide=hide_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(bars=False, share_y='row', log_y=False, norm=False):\n",
    "#     print(f'gen {bars}')\n",
    "    datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm', 'kyoto_binary']\n",
    "    print(share_y)\n",
    "    fig, axis = plt.subplots(len(datasets), 6, figsize=(12,9), tight_layout=True, sharey=share_y)\n",
    "    global title\n",
    "    global labels\n",
    "    global last\n",
    "    global twin\n",
    "    leg = False\n",
    "    labls = True\n",
    "    title = True\n",
    "    last = False\n",
    "    twin = axis[0][0]\n",
    "    for ds in datasets:\n",
    "        if datasets.index(ds) == (len(datasets) - 1):\n",
    "            last = True\n",
    "        dsdf = tdf[tdf.dataset == ds]\n",
    "#         display(dsdf)\n",
    "        aux_graphNx1(ds, axis, datasets.index(ds), dsdf, bar=bars, share_y=share_y, log_y=log_y, norm=norm)\n",
    "        title = False\n",
    "    lines_1, labels_1 = axis[0][0].get_legend_handles_labels()\n",
    "    lines_2, labels_2 = twin.get_legend_handles_labels()\n",
    "    lines = lines_1 + lines_2\n",
    "    labels = labels_1 + labels_2\n",
    "    lgd = fig.legend(lines, labels, loc=8, ncol=6, bbox_to_anchor=(0.5, -0.02))\n",
    "    lgd.set_in_layout(True)\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    filename = 'bars-all-4x1-JPI-delay.eps' if bars else 'all-4x1-JPI-delay.eps'\n",
    "    plt.savefig(f'Vostro-{filename}', pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = ['10', '50', '90']\n",
    "algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm','kyoto_binary']\n",
    "all_values = []\n",
    "# filtra algoritmo\n",
    "for alg in algs:\n",
    "#         line = alg + ' & $\\Delta$ '\n",
    "    line = alg + '  '\n",
    "    algdf = filter_by_substring_algorithm(tdf, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "    # filtra dataset\n",
    "    for ds in datasets:\n",
    "        dsalgdf = algdf[algdf.dataset == ds]\n",
    "#         display(dsalgdf.head())\n",
    "        # \"junta\"\n",
    "        for rt in rates:\n",
    "            if rt == '10':\n",
    "                showdf = rename_columns_by_rate(dsalgdf[dsalgdf.PERC == rt])\n",
    "            else:\n",
    "                to_join = dsalgdf[dsalgdf.PERC == rt]\n",
    "                showdf = showdf.merge(rename_columns_by_rate(to_join),on=['algorithm', 'dataset', 'batch_size', 'cores']).sort_values(['batch_size','cores'])\n",
    "#         showdf = showdf[['algorithm','dataset','cores','batch_size','JPI_10','JPI_50','JPI_90']]\n",
    "#         display(showdf)\n",
    "        for i in ['10', '50', '90']:\n",
    "            minoutro = min(showdf[showdf.batch_size == 1][f'JPI_{i}'])\n",
    "#             print(f\"JPI_{i} minoutro {minoutro}\")\n",
    "            val = ((showdf[showdf.batch_size != 1][f'JPI_{i}'].iloc[0] - minoutro) / minoutro ) * 100\n",
    "            all_values.append(val)\n",
    "            sval = f\"{val:.2f} \" if val < 0 else \"\\\\textbf{ \" + f\"{val:.2f}\" + \"} \"\n",
    "            line += f\"& {sval}\"\n",
    "    print(f\"{line} \\\\\\\\\")\n",
    "print(f'\\n\\nAverage reduction: {sum(all_values)/len(all_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_delta_rates_vert(ratio=False):\n",
    "    rates = ['10', '50', '90']\n",
    "    algs = ['Ada', 'L', 'Patches', 'Adwin', 'ASHT', 'OzaBag']\n",
    "    datasets = ['airlines', 'GMSC','elecNormNew','covtypeNorm','kyoto_binary']\n",
    "    all_values = []\n",
    "    # filtra algoritmo\n",
    "    for alg in algs:\n",
    "        line = '\\hline\\n\\\\multirow{3}{*}{' + alg + '} '\n",
    "        algdf = filter_by_substring_algorithm(tdf, alg).sort_values(['algorithm','batch_size','cores'])\n",
    "#         display(algdf)\n",
    "        # filtra rate\n",
    "        for rt in rates:\n",
    "            if rt != '10':\n",
    "                line += '\\\\\\\\\\n'\n",
    "            line += f' & {rt} '\n",
    "            rtalgdf = rename_columns_by_rate(algdf[algdf.PERC == rt])\n",
    "            # filtra dataset\n",
    "            for ds in datasets:\n",
    "#                 print(ds)\n",
    "                dsrtalgdf = rtalgdf[rtalgdf.dataset == ds]\n",
    "#                 display(dsrtalgdf)\n",
    "                minoutro = min(dsrtalgdf[dsrtalgdf.batch_size == 1][f'JPI_{rt}'])\n",
    "#                 print(f\"JPI_{rt} minoutro {minoutro}\")\n",
    "                val = ((dsrtalgdf[dsrtalgdf.batch_size != 1][f'JPI_{rt}'].iloc[0] - minoutro)/minoutro)*100 if ratio else dsrtalgdf[dsrtalgdf.batch_size != 1][f'JPI_{rt}'].iloc[0] - minoutro \n",
    "                all_values.append(val)\n",
    "                sval = f\"{val:.2f} \" if val < 0 else \"\\\\textbf{ \" + f\"{val:.2f}\" + \"} \"\n",
    "                line += f\"& {sval} \"\n",
    "        print(f\"{line} \\\\\\\\\")\n",
    "    print(f'\\n\\nAverage reduction: {sum(all_values)/len(all_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_delta_rates_vert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharey = row\n",
    "\n",
    "Linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = False\n",
    "\n",
    "linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "log scale y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "normalizado MB 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='90MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharey = false\n",
    "\n",
    "normalizado Seq10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='10S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_graph(bars=True, share_y=False, log_y=False, norm='10P')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
